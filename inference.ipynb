{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 경로 확인 및 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\KoBertSum\\\\src'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\KoBertSum\\src\n"
     ]
    }
   ],
   "source": [
    "cd ./src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 필요 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertConfig\n",
    "from torch.nn.init import xavier_uniform_\n",
    "\n",
    "from models.model_builder import *\n",
    "from models.encoder import *\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from prepro.tokenization_kobert import *\n",
    "from prepro.tokenization_kobert import KoBertTokenizer\n",
    "from kss import split_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 인자 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2bool(v):\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-encoder\", default='bert', type=str, choices=['bert', 'baseline'])\n",
    "parser.add_argument(\"-bert_data_path\", default='../bert_data_new/cnndm')\n",
    "parser.add_argument(\"-model_path\", default='../models/')\n",
    "parser.add_argument(\"-result_path\", default='../results/cnndm')\n",
    "parser.add_argument(\"-temp_dir\", default='../temp')\n",
    "\n",
    "parser.add_argument(\"-batch_size\", default=140, type=int)\n",
    "parser.add_argument(\"-test_batch_size\", default=200, type=int)\n",
    "\n",
    "parser.add_argument(\"-max_pos\", default=512, type=int)\n",
    "parser.add_argument(\"-use_interval\", type=str2bool, nargs='?',const=True,default=True)\n",
    "parser.add_argument(\"-large\", type=str2bool, nargs='?',const=True,default=False)\n",
    "parser.add_argument(\"-load_from_extractive\", default='', type=str)\n",
    "\n",
    "parser.add_argument(\"-sep_optim\", type=str2bool, nargs='?',const=True,default=False)\n",
    "parser.add_argument(\"-lr_bert\", default=2e-3, type=float)\n",
    "parser.add_argument(\"-lr_dec\", default=2e-3, type=float)\n",
    "parser.add_argument(\"-use_bert_emb\", type=str2bool, nargs='?',const=True,default=False)\n",
    "\n",
    "parser.add_argument(\"-share_emb\", type=str2bool, nargs='?', const=True, default=False)\n",
    "parser.add_argument(\"-finetune_bert\", type=str2bool, nargs='?', const=True, default=True)\n",
    "parser.add_argument(\"-dec_dropout\", default=0.2, type=float)\n",
    "parser.add_argument(\"-dec_layers\", default=6, type=int)\n",
    "parser.add_argument(\"-dec_hidden_size\", default=768, type=int)\n",
    "parser.add_argument(\"-dec_heads\", default=8, type=int)\n",
    "parser.add_argument(\"-dec_ff_size\", default=2048, type=int)\n",
    "parser.add_argument(\"-enc_hidden_size\", default=512, type=int)\n",
    "parser.add_argument(\"-enc_ff_size\", default=512, type=int)\n",
    "parser.add_argument(\"-enc_dropout\", default=0.2, type=float)\n",
    "parser.add_argument(\"-enc_layers\", default=6, type=int)\n",
    "\n",
    "parser.add_argument(\"-pretrained_model\", default='bert', type=str)\n",
    "\n",
    "parser.add_argument(\"-mode\", default='', type=str)\n",
    "parser.add_argument(\"-select_mode\", default='greedy', type=str)\n",
    "parser.add_argument(\"-map_path\", default='../../data/')\n",
    "parser.add_argument(\"-raw_path\", default='../../line_data')\n",
    "parser.add_argument(\"-save_path\", default='../../data/')\n",
    "\n",
    "parser.add_argument(\"-shard_size\", default=2000, type=int)\n",
    "parser.add_argument('-min_src_nsents', default=1, type=int)    # 3\n",
    "parser.add_argument('-max_src_nsents', default=120, type=int)    # 100\n",
    "parser.add_argument('-min_src_ntokens_per_sent', default=1, type=int)    # 5\n",
    "parser.add_argument('-max_src_ntokens_per_sent', default=300, type=int)    # 200\n",
    "parser.add_argument('-min_tgt_ntokens', default=1, type=int)    # 5\n",
    "parser.add_argument('-max_tgt_ntokens', default=500, type=int)    # 500\n",
    "\n",
    "parser.add_argument(\"-lower\", type=str2bool, nargs='?',const=True,default=True)\n",
    "parser.add_argument(\"-use_bert_basic_tokenizer\", type=str2bool, nargs='?',const=True,default=False)\n",
    "\n",
    "parser.add_argument('-log_file', default='../../logs/cnndm.log')\n",
    "\n",
    "parser.add_argument('-dataset', default='')\n",
    "\n",
    "parser.add_argument('-n_cpus', default=2, type=int)\n",
    "\n",
    "# params for EXT\n",
    "parser.add_argument(\"-ext_dropout\", default=0.2, type=float)\n",
    "parser.add_argument(\"-ext_layers\", default=2, type=int)\n",
    "parser.add_argument(\"-ext_hidden_size\", default=768, type=int)\n",
    "parser.add_argument(\"-ext_heads\", default=8, type=int)\n",
    "parser.add_argument(\"-ext_ff_size\", default=2048, type=int)\n",
    "\n",
    "parser.add_argument(\"-label_smoothing\", default=0.1, type=float)\n",
    "parser.add_argument(\"-generator_shard_size\", default=32, type=int)\n",
    "parser.add_argument(\"-alpha\",  default=0.6, type=float)\n",
    "parser.add_argument(\"-beam_size\", default=5, type=int)\n",
    "parser.add_argument(\"-min_length\", default=15, type=int)\n",
    "parser.add_argument(\"-max_length\", default=150, type=int)\n",
    "parser.add_argument(\"-max_tgt_len\", default=140, type=int)\n",
    "\n",
    "args = parser.parse_args('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BertData 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertData():\n",
    "    def __init__(self):\n",
    "        self.tokenizer = KoBertTokenizer.from_pretrained(\"monologg/kobert\", do_lower_case=True)\n",
    "\n",
    "        self.sep_token = '[SEP]'\n",
    "        self.cls_token = '[CLS]'\n",
    "        self.pad_token = '[PAD]'\n",
    "        self.sep_vid = self.tokenizer.token2idx[self.sep_token]\n",
    "        self.cls_vid = self.tokenizer.token2idx[self.cls_token]\n",
    "        self.pad_vid = self.tokenizer.token2idx[self.pad_token]\n",
    "\n",
    "    def preprocess(self, src):\n",
    "\n",
    "        if (len(src) == 0):\n",
    "            return None\n",
    "\n",
    "        original_src_txt = [' '.join(s) for s in src]\n",
    "        idxs = [i for i, s in enumerate(src) if (len(s) > 1)]\n",
    "\n",
    "        src = [src[i][:2000] for i in idxs]\n",
    "        src = src[:1000]\n",
    "\n",
    "        if (len(src) < 3):\n",
    "            return None\n",
    "\n",
    "        src_txt = [' '.join(sent) for sent in src]\n",
    "        text = ' [SEP] [CLS] '.join(src_txt)\n",
    "        src_subtokens = self.tokenizer.tokenize(text)\n",
    "        src_subtokens = src_subtokens[:1022]  ## 512가 최대인데 [SEP], [CLS] 2개 때문에 510\n",
    "        src_subtokens = ['[CLS]'] + src_subtokens + ['[SEP]']\n",
    "\n",
    "        src_subtoken_idxs = self.tokenizer.convert_tokens_to_ids(src_subtokens)\n",
    "        _segs = [-1] + [i for i, t in enumerate(src_subtoken_idxs) if t == self.sep_vid]\n",
    "        segs = [_segs[i] - _segs[i - 1] for i in range(1, len(_segs))]\n",
    "        segments_ids = []\n",
    "        for i, s in enumerate(segs):\n",
    "            if (i % 2 == 0):\n",
    "                segments_ids += s * [0]\n",
    "            else:\n",
    "                segments_ids += s * [1]\n",
    "        cls_ids = [i for i, t in enumerate(src_subtoken_idxs) if t == self.cls_vid]\n",
    "        labels = None\n",
    "        src_txt = [original_src_txt[i] for i in idxs]\n",
    "        tgt_txt = None\n",
    "        \n",
    "        return src_subtoken_idxs, labels, segments_ids, cls_ids, src_txt, tgt_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout, dim, max_len=5000):\n",
    "        pe = torch.zeros(max_len, dim)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp((torch.arange(0, dim, 2, dtype=torch.float) *\n",
    "                              -(math.log(10000.0) / dim)))\n",
    "        pe[:, 0::2] = torch.sin(position.float() * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position.float() * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.register_buffer('pe', pe)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, emb, step=None):\n",
    "        emb = emb * math.sqrt(self.dim)\n",
    "        if (step):\n",
    "            emb = emb + self.pe[:, step][:, None, :]\n",
    "\n",
    "        else:\n",
    "            emb = emb + self.pe[:, :emb.size(1)]\n",
    "        emb = self.dropout(emb)\n",
    "        return emb\n",
    "\n",
    "    def get_emb(self, emb):\n",
    "        return self.pe[:, :emb.size(1)]\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, d_ff, dropout):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "\n",
    "        self.self_attn = MultiHeadedAttention(\n",
    "            heads, d_model, dropout=dropout)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, iter, query, inputs, mask):\n",
    "        if (iter != 0):\n",
    "            input_norm = self.layer_norm(inputs)\n",
    "        else:\n",
    "            input_norm = inputs\n",
    "\n",
    "        mask = mask.unsqueeze(1)\n",
    "        context = self.self_attn(input_norm, input_norm, input_norm,\n",
    "                                 mask=mask)\n",
    "        out = self.dropout(context) + inputs\n",
    "        return self.feed_forward(out)\n",
    "\n",
    "class ExtTransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, heads, dropout, num_inter_layers=0):\n",
    "        super(ExtTransformerEncoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_inter_layers = num_inter_layers\n",
    "        self.pos_emb = PositionalEncoding(dropout, d_model)\n",
    "        self.transformer_inter = nn.ModuleList(\n",
    "            [TransformerEncoderLayer(d_model, heads, d_ff, dropout)\n",
    "             for _ in range(num_inter_layers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.wo = nn.Linear(d_model, 1, bias=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, top_vecs, mask):\n",
    "        \"\"\" See :obj:`EncoderBase.forward()`\"\"\"\n",
    "\n",
    "        batch_size, n_sents = top_vecs.size(0), top_vecs.size(1)\n",
    "        pos_emb = self.pos_emb.pe[:, :n_sents]\n",
    "        x = top_vecs * mask[:, :, None].float()\n",
    "        x = x + pos_emb\n",
    "\n",
    "        for i in range(self.num_inter_layers):\n",
    "            x = self.transformer_inter[i](i, x, x, ~mask)  # all_sents * max_tokens * dim\n",
    "\n",
    "        x = self.layer_norm(x)\n",
    "        sent_scores = self.sigmoid(self.wo(x))\n",
    "        sent_scores = sent_scores.squeeze(-1) * mask.float()\n",
    "\n",
    "        return sent_scores\n",
    "    \n",
    "class Bert(nn.Module):\n",
    "    temp_dir = 'D:/KoBertSum/temp'\n",
    "    def __init__(self, large, temp_dir, finetune=False):\n",
    "        super(Bert, self).__init__()\n",
    "        self.model = BertModel.from_pretrained(\"monologg/kobert\", cache_dir=temp_dir)\n",
    "\n",
    "        self.finetune = finetune\n",
    "\n",
    "    def forward(self, x, segs, mask):\n",
    "        top_vec = self.model(input_ids=x, token_type_ids=segs, attention_mask=mask)[0]\n",
    "\n",
    "        return top_vec\n",
    "    \n",
    "class ExtSummarizer(nn.Module):\n",
    "    def __init__(self, args, device, checkpoint):\n",
    "        super(ExtSummarizer, self).__init__()\n",
    "        self.args = args\n",
    "        self.device = device\n",
    "        self.bert = Bert(args.large, args.temp_dir)\n",
    "\n",
    "        self.ext_layer = ExtTransformerEncoder(self.bert.model.config.hidden_size, args.ext_ff_size, args.ext_heads,\n",
    "                                               args.ext_dropout, args.ext_layers)\n",
    "        if (args.encoder == 'baseline'):\n",
    "            bert_config = BertConfig(self.bert.model.config.vocab_size, hidden_size=args.ext_hidden_size,\n",
    "                                     num_hidden_layers=args.ext_layers, num_attention_heads=args.ext_heads, intermediate_size=args.ext_ff_size)\n",
    "            self.bert.model = BertModel(bert_config)\n",
    "            self.ext_layer = Classifier(self.bert.model.config.hidden_size)\n",
    "\n",
    "        if(args.max_pos>512):\n",
    "            my_pos_embeddings = nn.Embedding(args.max_pos, self.bert.model.config.hidden_size)\n",
    "            my_pos_embeddings.weight.data[:512] = self.bert.model.embeddings.position_embeddings.weight.data\n",
    "            my_pos_embeddings.weight.data[512:] = self.bert.model.embeddings.position_embeddings.weight.data[-1][None,:].repeat(args.max_pos-512,1)\n",
    "            self.bert.model.embeddings.position_embeddings = my_pos_embeddings\n",
    "\n",
    "\n",
    "        if checkpoint is not None:\n",
    "            self.load_state_dict(checkpoint['model'], strict=True)\n",
    "        else:\n",
    "            if args.param_init != 0.0:\n",
    "                for p in self.ext_layer.parameters():\n",
    "                    p.data.uniform_(-args.param_init, args.param_init)\n",
    "            if args.param_init_glorot:\n",
    "                for p in self.ext_layer.parameters():\n",
    "                    if p.dim() > 1:\n",
    "                        xavier_uniform_(p)\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, src, segs, clss, mask_src, mask_cls):\n",
    "        top_vec = self.bert(src, segs, mask_src)\n",
    "        #print(top_vec)\n",
    "        #top_vec = top_vec.last_hidden_state\n",
    "        sents_vec = top_vec[torch.arange(top_vec.size(0)).unsqueeze(1), clss]\n",
    "        sents_vec = sents_vec * mask_cls[:, :, None].float()\n",
    "        sent_scores = self.ext_layer(sents_vec, mask_cls).squeeze(-1)\n",
    "        return sent_scores, mask_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(text):\n",
    "    \n",
    "    def txt2input(text):\n",
    "        #data = list(filter(None, text.split('\\n')))\n",
    "        #data = split_sentences(text)\n",
    "        bertdata = BertData()\n",
    "        txt_data = bertdata.preprocess(text)\n",
    "        data_dict = {\"src\":txt_data[0],\n",
    "                    \"labels\":[0,1,2],\n",
    "                    \"segs\":txt_data[2],\n",
    "                    \"clss\":txt_data[3],\n",
    "                    \"src_txt\":txt_data[4],\n",
    "                    \"tgt_txt\":None}\n",
    "        input_data = []\n",
    "        input_data.append(data_dict)\n",
    "        return input_data\n",
    "    \n",
    "    input_data = txt2input(text)\n",
    "    device = torch.device(\"cuda\")\n",
    "    \n",
    "    def _pad(data, pad_id, width=-1):\n",
    "        if (width == -1):\n",
    "            width = max(len(d) for d in data)\n",
    "        rtn_data = [d + [pad_id] * (width - len(d)) for d in data]\n",
    "        return rtn_data\n",
    "    \n",
    "    pre_src = [x['src'] for x in input_data]\n",
    "    pre_segs = [x['segs'] for x in input_data]\n",
    "    pre_clss = [x['clss'] for x in input_data]\n",
    "\n",
    "    src = torch.tensor(_pad(pre_src, 0)).cuda()\n",
    "    segs = torch.tensor(_pad(pre_segs, 0)).cuda()\n",
    "    mask_src = ~(src == 0)\n",
    "\n",
    "    clss = torch.tensor(_pad(pre_clss, -1)).cuda()\n",
    "    mask_cls = ~(clss == -1)\n",
    "    clss[clss == -1] = 0\n",
    "\n",
    "    clss.to(device).long()\n",
    "    mask_cls.to(device).long()\n",
    "    segs.to(device).long()\n",
    "    mask_src.to(device).long()\n",
    "    \n",
    "    #checkpoint = torch.load(\"D:/KoBertSum/ext/models/model_step_26000.pt\")  # V2\n",
    "    #checkpoint = torch.load(\"D:/KoBertSum/ext/models/model_step_5000.pt\")  # V1\n",
    "    checkpoint = torch.load(\"D:/KoBertSum/ext/models/model_step_5000_2.pt\")  # V3 - max_pos 1024\n",
    "    model = ExtSummarizer(args, device, checkpoint)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        sent_scores, mask = model(src, segs, clss, mask_src, mask_cls)\n",
    "        sent_scores = sent_scores + mask.float()\n",
    "        sent_scores = sent_scores.cpu().data.numpy()\n",
    "        print(sent_scores)\n",
    "        selected_ids = np.argsort(-sent_scores, 1)\n",
    "        print(selected_ids)\n",
    "    \n",
    "    return selected_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "\"뭔가 있긴 있는 거 같은데, 아직은 완벽하지 않은 것 같다.\"\n",
    "\n",
    "국내 연구진이 개발한 상온 초전도체 'LK-99'의 정체에 대해 국내 과학자들은 말끝을 흐리며 이같이 애매모호하게 답했다. 국내외에서 재현 실험 등의 검증이 이뤄지고 있는 상황에서 확실하게 말할 수 없음을 이해해 달라고도 했다. 그럼에도 이전까지 볼 수 없었던 물질임에는 틀림 없다는 여운을 남겼다.\n",
    "\n",
    "취재 과정에서 인터뷰한 대다수 과학자들은 LK-99가 초전도 특성을 가진 물질인 거 같긴 한데, 그렇다고 100% 초전도체라고 현재로선 단정적으로 말하기 어렵다는 얘기를 공통적으로 꺼냈다.\n",
    "\n",
    "초전도체 검증 결과 여부를 떠나 21세기 들어 이처럼 새로운 과학적 발명에 이토록 전 세계가 떠들석하고, 가히 센세이셔널한 반응을 몰고 온 적이 있었던가. 아마도 현대과학의 혁명기로, 과학적 발명과 발견이 많았던 19세기 말과 20세기 초를 제외하곤 이번이 처음인 듯 싶다. 그만큼 인류가 얼마나 세기적 발명의 탄생을 목말라 했는지를 가늠케 한 사건이었다.\n",
    "\n",
    "지난달 22일 퀀텀에너지연구소가 상온·상압 초전도체 'LK-99' 개발에 대한 논문을 사전 공개 사이트(아카이브)에 올린 이후 지금까지 전 세계가 우리나라를 주목하고 있다. 112년 전인 1911년 네덜란드 라이덴 대학의 카멜린 온네스 교수가 초전도 현상을 발견한 이후, 상온 초전도체 개발에 수많은 과학자들이 밤낮을 잊고 실험실을 지켜 왔지만 모두 허사로 끝났다.\n",
    "\n",
    "이렇듯 '꿈의 물질'로 불리는 상온 초전도체 개발은 노벨상 '0순위'이자, 기존 기술패권 질서의 판을 한 순간에 뒤집어 놓을 파괴적 혁신기술로 추앙 받아왔다. 그토록 갈망하던 현대 과학계의 난제 중의 하나인 상온 초전도체 개발 소식이 한국에서 전해진 것이다.\n",
    "\n",
    "그것도 세계 유수의 대학과 연구소, 글로벌 대기업이 아닌 2008년 창업한 대학 실험실 기반 벤처기업이 깜짝 주인공으로 등장했다. 이 기업은 국내 상온·상압 초전도체 연구의 토대를 놓은 초전도 이론의 대가로 불리는 고(故) 최동식 고려대 교수의 제자들이 주축이 돼 설립됐다. 이들의 논문에 '고 최동식 교수를 기립니다'는 문장이 기재돼 있을 정도로, 대학 시절부터 20년 넘게 스승의 유훈을 받들어 상온 초전도체 개발을 이어 왔다.\n",
    "\n",
    "LK-99는 이석배(L) 대표와 김지훈(K) 연구소장의 영문 이니셜을 따서 정했고, LK-99를 발견해 본격적인 연구를 시작한 1999년을 기념해 '99'라고 붙여졌다. 이들은 지난 20년 동안 1000회 이상의 실험을 반복한 끝에 LK-99 개발에 성공했다고 했다.\n",
    "\n",
    "LK-99 진위 여부를 확인하기 위해 전 세계 주류 연구그룹들이 일을 제쳐놓고 재현 실험 등을 통한 검증에 힘을 쏟고 있다. 글로벌 빅테크 기업도 이에 합류했다는 소식도 들린다. 지금까지의 결과를 보면 LK-99는 상온 초전도체가 아닐 가능성이 커 보인다. 더 많은 검증 결과가 나올수록 한국발 '상온 초전도체 탄생'은 물거품이 될 가능성이 높아 보인다. 미 프린스턴대는 LK-99가 자석일 가능성이 크다고 밝혔다.\n",
    "\n",
    "상온 초전도체에 대한 회의적 시각이 늘면서 LK-99 검증 논란을 '제2의 황우석 사태'에 빗대어 깎아 내릴 듯 하다. 분명한 것은 논문을 고의로 조작한 황우석 사태와 LK-99는 본질적으로 다르다. 이들의 논문 데이터가 다소 부실해 보여도 데이터 조작 흔적은 없다는 게 과학자들의 공통된 주장이다.\n",
    "\n",
    "여기에 줄기세포 논란에 거짓 대응한 황우석과 달리 이들은 과학계의 검증에 열린 마음으로 대응하고 있다는 점도 커다란 차이다. 비록 LK-99가 상온 초전도체가 아니더라도 우리가 떳떳하지 못할 이유가 없다. 이전에 발견하지 못했던 새로운 물질이라는 점만 입증돼도 이들의 노력은 충분히 높게 평가 받아야 한다. 결코 황우석 사태와 비교해 비난해선 안 될 것이다.\n",
    "\n",
    "과학은 무수한 실패 과정을 거쳐 진보해 왔다. 그렇기에 과학은 솔직해야 하고, 거짓이 없어야 한다. LK-99가 신기루에 불과하더라도 우리의 작은 벤처가 촉발시킨 상온 초전도체 이슈는 전 세계 과학자들의 개발 경쟁을 가속화해 우리를 더 나은 세상으로 이끌어 주는 트리거가 될 것이라는 점만으로도 의미있는 일이 아닐까. 우리나라가 초전도체 개발을 계속 이어가야 하는 이유이기도 하다.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"뭔가 있긴 있는 거 같은데, 아직은 완벽하지 않은 것 같다.\"',\n",
       " \"국내 연구진이 개발한 상온 초전도체 'LK-99'의 정체에 대해 국내 과학자들은 말끝을 흐리며 이같이 애매모호하게 답했다.\",\n",
       " '국내외에서 재현 실험 등의 검증이 이뤄지고 있는 상황에서 확실하게 말할 수 없음을 이해해 달라고도 했다.',\n",
       " '그럼에도 이전까지 볼 수 없었던 물질임에는 틀림 없다는 여운을 남겼다.',\n",
       " '취재 과정에서 인터뷰한 대다수 과학자들은 LK-99가 초전도 특성을 가진 물질인 거 같긴 한데, 그렇다고 100% 초전도체라고 현재로선 단정적으로 말하기 어렵다는 얘기를 공통적으로 꺼냈다.',\n",
       " '초전도체 검증 결과 여부를 떠나 21세기 들어 이처럼 새로운 과학적 발명에 이토록 전 세계가 떠들석하고, 가히 센세이셔',\n",
       " '널한 반응을 몰고 온 적이 있었던가.',\n",
       " '아마도 현대과학의 혁명기로, 과학적 발명과 발견이 많았던 19세기 말과 20세기 초를 제외하곤 이번이 처음인 듯 싶다.',\n",
       " '그만큼 인류가 얼마나 세기적 발명의 탄생을 목말라 했는지를 가늠케 한 사건이었다.',\n",
       " \"지난달 22일 퀀텀에너지연구소가 상온·상압 초전도체 'LK-99' 개발에 대한 논문을 사전 공개 사이트(아카이브)에 올린 이후 지금까지 전 세계가 우리나라를 주목하고 있다.\",\n",
       " '112년 전인 1911년 네덜란드 라이덴 대학의 카멜린 온네스 교수가 초전도 현상을 발견한 이후, 상온 초전도체 개발에 수많은 과학자들이 밤낮을 잊고 실험실을 지켜 왔지만 모두 허사로 끝났다.',\n",
       " \"이렇듯 '꿈의 물질'로 불리는 상온 초전도체 개발은 노벨상 '0순위'이자, 기존 기술패권 질서의 판을 한 순간에 뒤집어 놓을 파괴적 혁신기술로 추앙 받아왔다.\",\n",
       " '그토록 갈망하던 현대 과학계의 난제 중의 하나인 상온 초전도체 개발 소식이 한국에서 전해진 것이다.',\n",
       " '그것도 세계 유수의 대학과 연구소, 글로벌 대기업이 아닌 2008년 창업한 대학 실험실 기반 벤처기업이 깜짝 주인공으로 등장했다.',\n",
       " '이 기업은 국내 상온·상압 초전도체 연구의 토대를 놓은 초전도 이론의 대가로 불리는 고(故) 최동식 고려대 교수의 제자들이 주축이 돼 설립됐다.',\n",
       " \"이들의 논문에 '고 최동식 교수를 기립니다'는 문장이 기재돼 있을 정도로, 대학 시절부터 20년 넘게 스승의 유훈을 받들어 상온 초전도체 개발을 이어 왔다.\",\n",
       " \"LK-99는 이석배(L) 대표와 김지훈(K) 연구소장의 영문 이니셜을 따서 정했고, LK-99를 발견해 본격적인 연구를 시작한 1999년을 기념해 '99'라고 붙여졌다.\",\n",
       " '이들은 지난 20년 동안 1000회 이상의 실험을 반복한 끝에 LK-99 개발에 성공했다고 했다.',\n",
       " 'LK-99 진위 여부를 확인하기 위해 전 세계 주류 연구그룹들이 일을 제쳐놓고 재현 실험 등을 통한 검증에 힘을 쏟고 있다.',\n",
       " '글로벌 빅테크 기업도 이에 합류했다는 소식도 들린다.',\n",
       " '지금까지의 결과를 보면 LK-99는 상온 초전도체가 아닐 가능성이 커 보인다.',\n",
       " \"더 많은 검증 결과가 나올수록 한국발 '상온 초전도체 탄생'은 물거품이 될 가능성이 높아 보인다.\",\n",
       " '미 프린스턴대는 LK-99가 자석일 가능성이 크다고 밝혔다.',\n",
       " \"상온 초전도체에 대한 회의적 시각이 늘면서 LK-99 검증 논란을 '제2의 황우석 사태'에 빗대어 깎아 내릴 듯 하다.\",\n",
       " '분명한 것은 논문을 고의로 조작한 황우석 사태와 LK-99는 본질적으로 다르다.',\n",
       " '이들의 논문 데이터가 다소 부실해 보여도 데이터 조작 흔적은 없다는 게 과학자들의 공통된 주장이다.',\n",
       " '여기에 줄기세포 논란에 거짓 대응한 황우석과 달리 이들은 과학계의 검증에 열린 마음으로 대응하고 있다는 점도 커다란 차이다.',\n",
       " '비록 LK-99가 상온 초전도체가 아니더라도 우리가 떳떳하지 못할 이유가 없다.',\n",
       " '이전에 발견하지 못했던 새로운 물질이라는 점만 입증돼도 이들의 노력은 충분히 높게 평가 받아야 한다.',\n",
       " '결코 황우석 사태와 비교해 비난해선 안 될 것이다.',\n",
       " '과학은 무수한 실패 과정을 거쳐 진보해 왔다.',\n",
       " '그렇기에 과학은 솔직해야 하고, 거짓이 없어야 한다.',\n",
       " 'LK-99가 신기루에 불과하더라도 우리의 작은 벤처가 촉발시킨 상온 초전도체 이슈는 전 세계 과학자들의 개발 경쟁을 가속화해 우리를 더 나은 세상으로 이끌어 주는 트리거가 될 것이라는 점만으로도 의미있는 일이 아닐까.',\n",
       " '우리나라가 초전도체 개발을 계속 이어가야 하는 이유이기도 하다.']"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import kss\n",
    "text = kss.split_sentences(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ['(서울=연합뉴스) 이주영 기자 = 빠른 진화 속도로 혹독한 환경에 적응하며 4억년을 살아온 히말라야 티베트고원의 이끼도 현재 진행되고 있는 지구온난화에는 살아남지 못할 것이라는 연구 결과가 나왔다.',\n",
    "\"독일 프라이부르크대 랄프 레스키 교수와 중국 서우두사범대 허이쿤 교수팀은 10일 과학저널 '셀'(Cell)에서 티베트고원 등에 사는 화석 식물인 타카키아 이끼의 DNA 분석 결과 유전적으로 매우 빠른 진화 특성을 가졌지만 현 기후변화에서 살아남을 만큼 빠르게 진화하지는 못할 가능성이 큰 것으로 나타났다고 밝혔다.\",\n",
    "'티베트고원 얼음 절벽에서 3억9천만년이나 살아온 타카키아는 작고 느리게 자라는 이끼로 히말라야 4천ｍ 고지대와 일본, 미국 등 일부 지역에서만 볼 수 있다.',\n",
    "'연구팀은 티베트고원의 타카키아 서식지를 10년간 18차례 방문해 샘플을 수집하고 서식지를 조사했다.',\n",
    "'타카키아의 DNA 염기서열을 분석하고 기후변화가 타카키아에 어떤 영향을 미치는지 연구했다.',\n",
    "'레스키 교수는 지각변동으로 히말라야산맥이 솟아올랐을 때는 타카키아가 등장한 지 1억년이 지난 시점이었고, 이런 급격한 환경 변화 속에서 살아남았다며 이 연구를 통해 그 비밀을 밝히고자 했다고 말했다.',\n",
    "'연구팀은 DNA 분석 결과 타카키아의 게놈(유전체)이 여러 세대에 걸쳐 자연선택에 의해 진화하면서 손상된 DNA를 고치고 자외선 손상으로부터 회복하는 데 탁월한 유전자들을 많이 갖게 된 것으로 나타났다고 밝혔다.',\n",
    "'레스키 교스는 \"타카키아가 현재 빠르게 진화하는 유전자가 가장 많은 게놈을 가지고 있다는 사실을 발견했다\"고 말했다.',\n",
    "'허이쿤 교수는 \"타카키아는 매년 8개월간 눈에 덮여 있고 4개월은 고강도 자외선을 받는다\"며 \"타카키아는 이에 대응해 유연한 가지 뻗기로 다양한 위치에서 살 수 있게 적응했고 이를 통해 폭설과 자외선을 견딜 수 있는 견고한 개체군 구조를 만들었다\"고 설명했다.',\n",
    "'연구팀은 또 타카키아 분류에 대해서도 이끼인지, 조류인지 등에 대한 논란이 있었으나 이번 게놈 분석으로 이끼라는 게 확인됐다며 시간 흐름에 따라 게놈이 크게 변했음에도 식물체 형태가 거의 변하지 않은 점은 새 연구 과제라고 밝혔다.',\n",
    "'연구팀은 그러나 타카키아가 과거 환경변화에 빠르게 적응해 살아남았지만 현재의 온난화와 서식지 감소 속도를 고려하면 앞으로 100년 이상 살아남기는 어려울 것으로 보인다고 우려했다.',\n",
    "'연구가 진행되는 동안 티베트고원의 타카키아 개체수는 매년 1.6%씩 감소했으며 서식지도 빠르게 줄어 금세기 말에는 타카키아에 적합한 서식지가 세계적으로 1천~1천500㎢밖에 남지 않을 것으로 예상됐다.',\n",
    "'연구팀은 타카키아의 멸종을 막기 위해 실험실에서 타카키아를 증식한 다음 티베트고원에 이식하는 시도를 하고 있으며 5년간 관찰 결과 이식된 식물 일부가 생존하고 번성하는 것으로 나타났다고 밝혔다.',\n",
    "'레스키 교수는 \"인간이 진화 정점에 있다고 생각하지만 공룡도 왔다가 사라진 것처럼 인간도 사라질 수 있다\"며 \"공룡의 등장과 멸종, 인간의 등장을 지켜본 타카키아로부터 회복력과 멸종에 대해 무언가를 배울 수 있을 것\"이라고 말했다.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'KoBertTokenizer'.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (1024) must match the size of tensor b (512) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[454], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m result \u001b[39m=\u001b[39m summarize(text)\n",
      "Cell \u001b[1;32mIn[452], line 51\u001b[0m, in \u001b[0;36msummarize\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     48\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[0;32m     50\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m---> 51\u001b[0m     sent_scores, mask \u001b[39m=\u001b[39m model(src, segs, clss, mask_src, mask_cls)\n\u001b[0;32m     52\u001b[0m     sent_scores \u001b[39m=\u001b[39m sent_scores \u001b[39m+\u001b[39m mask\u001b[39m.\u001b[39mfloat()\n\u001b[0;32m     53\u001b[0m     sent_scores \u001b[39m=\u001b[39m sent_scores\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mnumpy()\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\mecab\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[451], line 131\u001b[0m, in \u001b[0;36mExtSummarizer.forward\u001b[1;34m(self, src, segs, clss, mask_src, mask_cls)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, src, segs, clss, mask_src, mask_cls):\n\u001b[1;32m--> 131\u001b[0m     top_vec \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(src, segs, mask_src)\n\u001b[0;32m    132\u001b[0m     \u001b[39m#print(top_vec)\u001b[39;00m\n\u001b[0;32m    133\u001b[0m     \u001b[39m#top_vec = top_vec.last_hidden_state\u001b[39;00m\n\u001b[0;32m    134\u001b[0m     sents_vec \u001b[39m=\u001b[39m top_vec[torch\u001b[39m.\u001b[39marange(top_vec\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m))\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m), clss]\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\mecab\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[451], line 91\u001b[0m, in \u001b[0;36mBert.forward\u001b[1;34m(self, x, segs, mask)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, segs, mask):\n\u001b[1;32m---> 91\u001b[0m     top_vec \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(input_ids\u001b[39m=\u001b[39;49mx, token_type_ids\u001b[39m=\u001b[39;49msegs, attention_mask\u001b[39m=\u001b[39;49mmask)[\u001b[39m0\u001b[39m]\n\u001b[0;32m     93\u001b[0m     \u001b[39mreturn\u001b[39;00m top_vec\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\mecab\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\mecab\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1015\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1008\u001b[0m \u001b[39m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m   1009\u001b[0m \u001b[39m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m   1010\u001b[0m \u001b[39m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m   1011\u001b[0m \u001b[39m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m   1012\u001b[0m \u001b[39m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m   1013\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m-> 1015\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings(\n\u001b[0;32m   1016\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[0;32m   1017\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m   1018\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m   1019\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   1020\u001b[0m     past_key_values_length\u001b[39m=\u001b[39;49mpast_key_values_length,\n\u001b[0;32m   1021\u001b[0m )\n\u001b[0;32m   1022\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(\n\u001b[0;32m   1023\u001b[0m     embedding_output,\n\u001b[0;32m   1024\u001b[0m     attention_mask\u001b[39m=\u001b[39mextended_attention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1032\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[0;32m   1033\u001b[0m )\n\u001b[0;32m   1034\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\mecab\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\mecab\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:238\u001b[0m, in \u001b[0;36mBertEmbeddings.forward\u001b[1;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embedding_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mabsolute\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    237\u001b[0m     position_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embeddings(position_ids)\n\u001b[1;32m--> 238\u001b[0m     embeddings \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m position_embeddings\n\u001b[0;32m    239\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLayerNorm(embeddings)\n\u001b[0;32m    240\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(embeddings)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (1024) must match the size of tensor b (512) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "result = summarize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'KoBertTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.2912345 1.35289   1.3499987 1.3547711 1.2963245 1.2898964]]\n",
      "[[3 1 2 4 0 5]]\n"
     ]
    }
   ],
   "source": [
    "result = summarize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['연구팀은 티베트고원의 타카키아 서식지를 10년간 18차례 방문해 샘플을 수집하고 서식지를 조사했다.',\n",
       " \"독일 프라이부르크대 랄프 레스키 교수와 중국 서우두사범대 허이쿤 교수팀은 10일 과학저널 '셀'(Cell)에서 티베트고원 등에 사는 화석 식물인 타카키아 이끼의 DNA 분석 결과 유전적으로 매우 빠른 진화 특성을 가졌지만 현 기후변화에서 살아남을 만큼 빠르게 진화하지는 못할 가능성이 큰 것으로 나타났다고 밝혔다.\",\n",
       " '티베트고원 얼음 절벽에서 3억9천만년이나 살아온 타카키아는 작고 느리게 자라는 이끼로 히말라야 4천ｍ 고지대와 일본, 미국 등 일부 지역에서만 볼 수 있다.',\n",
       " '타카키아의 DNA 염기서열을 분석하고 기후변화가 타카키아에 어떤 영향을 미치는지 연구했다.']"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[text[i] for i in result[0][:len(text)//3]]\n",
    "#[text[i] for i in result[0][:3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 탐구"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ['BNK경남은행 간부의 562억원 횡령 사건이 채 잊히기도 전인데 또 다른 금융사고가 연발하고 있다.',\n",
    "'수법도 거래 기업의 미공개 정보로 주식을 거래해 부당이득을 취하고, 고객 몰래 계좌를 개설하는 등 눈앞의 이익을 위해 온갖 불법·편법 행위가 동원되고 있다.',\n",
    "'KB국민은행의 직원들이 고객사의 미공개정보를 이용해 주식을 사고팔아 127억원의 부당이득을 챙긴 사실이 최근 금융당국에 적발됐다.',\n",
    "'이들은 2021년 1월부터 올해 4월까지 61개 상장사의 증권 업무를 대행하며 알게 된 무상증자 규모와 일정을 주식 매수에 이용했다.',\n",
    "'정보 공개 전 미리 주식을 사뒀다가 공시 뒤 주가가 오르면 팔았다.',\n",
    "'무상증자를 하게 되면 기업재무구조가 건전한 것으로 풀이돼 주가에는 호재로 작용한다.',\n",
    "'이런 방식으로 66억원 정도를 챙겼고, 일부는 다른 부서의 동료나 가족 등에게도 정보를 전달했다.',\n",
    "'이 과정에서도 61억원 상당의 부당 이득이 발생했다.',\n",
    "'대구은행 일부 지점 직원 수십명은 평가 실적을 올리기 위해 지난해 1000여건이 넘는 고객 문서를 위조해 증권 계좌를 개설한 것으로 파악됐다.',\n",
    "'이 직원들은 내점한 고객을 상대로 증권사 연계 계좌를 만들어 달라고 요청한 뒤 해당 계좌 신청서를 복사해 고객의 동의 없이 같은 증권사의 계좌를 하나 더 만들었다.',\n",
    "'A증권사 위탁 계좌 개설 신청서를 받고, 같은 신청서를 복사해 A증권사 해외선물계좌까지 개설하는 방식이다.',\n",
    "'최근 한 고객이 동의하지 않은 계좌가 개설됐다는 사실을 알게 돼 대구은행에 민원을 제기하면서 직원들의 비리가 드러났다.',\n",
    "'대구은행은 문제를 인지하고도 금감원에 이 사실을 보고하지 않았고, 영업점들에 공문을 보내 불건전 영업행위를 예방하라고 안내하는 데 그쳤다.',\n",
    "'금융실명제법 위반, 사문서 위조 등에 해당할 수 있는 범죄행위를 대수롭지 않게 넘기는 안일함이 혀를 차게 한다.',\n",
    "'국내 은행은 땅 짚고 헤엄치기식 이자장사로 평균 1억원대 고연봉을 누리는 직종이다.',\n",
    "'시중은행은 미국발 고금리에 편승해 거둬들인 막대한 예대마진으로 최근 수년간 성과급 잔치를 벌여 국민의 눈총을 받았다.',\n",
    "'국민의 재산으로 손쉽게 수익을 올리는 직종이라면 누구보다 엄격한 도덕적 기준을 세워도 모자랄 판에 ‘내 몫을 더 챙기겠다’며 이기적 탐욕을 부리고 있으니 말문이 막힌다.',\n",
    "'자체 내부통제가 안 된다면 현행 솜방망이 처벌 수위를 높이는 수밖에 없다.',\n",
    "'주요 동기가 경제적 이익인 만큼 벌금이나 과징금, 양형 부과 수준을 크게 높여 법 무서운 줄 알도록 해야 한다.',\n",
    "'주요 선진국에서 이미 도입한 불공정거래 범죄자에 대한 자본시장 거래제한제도도 적극 검토할 필요가 있다.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(서울=연합뉴스) 이주영 기자 = 빠른 진화 속도로 혹독한 환경에 적응하며 4억년을 살아온 히말라야 티베트고원의 이끼도 현재 진행되고 있는 지구온난화에는 살아남지 못할 것이라는 연구 결과가 나왔다.',\n",
       " \"독일 프라이부르크대 랄프 레스키 교수와 중국 서우두사범대 허이쿤 교수팀은 10일 과학저널 '셀'(Cell)에서 티베트고원 등에 사는 화석 식물인 타카키아 이끼의 DNA 분석 결과 유전적으로 매우 빠른 진화 특성을 가졌지만 현 기후변화에서 살아남을 만큼 빠르게 진화하지는 못할 가능성이 큰 것으로 나타났다고 밝혔다.\",\n",
       " '티베트고원 얼음 절벽에서 3억9천만년이나 살아온 타카키아는 작고 느리게 자라는 이끼로 히말라야 4천ｍ 고지대와 일본, 미국 등 일부 지역에서만 볼 수 있다.',\n",
       " '연구팀은 티베트고원의 타카키아 서식지를 10년간 18차례 방문해 샘플을 수집하고 서식지를 조사했다.',\n",
       " '타카키아의 DNA 염기서열을 분석하고 기후변화가 타카키아에 어떤 영향을 미치는지 연구했다.',\n",
       " '레스키 교수는 지각변동으로 히말라야산맥이 솟아올랐을 때는 타카키아가 등장한 지 1억년이 지난 시점이었고, 이런 급격한 환경 변화 속에서 살아남았다며 이 연구를 통해 그 비밀을 밝히고자 했다고 말했다.',\n",
       " '연구팀은 DNA 분석 결과 타카키아의 게놈(유전체)이 여러 세대에 걸쳐 자연선택에 의해 진화하면서 손상된 DNA를 고치고 자외선 손상으로부터 회복하는 데 탁월한 유전자들을 많이 갖게 된 것으로 나타났다고 밝혔다.',\n",
       " '레스키 교스는 \"타카키아가 현재 빠르게 진화하는 유전자가 가장 많은 게놈을 가지고 있다는 사실을 발견했다\"고 말했다.',\n",
       " '허이쿤 교수는 \"타카키아는 매년 8개월간 눈에 덮여 있고 4개월은 고강도 자외선을 받는다\"며 \"타카키아는 이에 대응해 유연한 가지 뻗기로 다양한 위치에서 살 수 있게 적응했고 이를 통해 폭설과 자외선을 견딜 수 있는 견고한 개체군 구조를 만들었다\"고 설명했다.',\n",
       " '연구팀은 또 타카키아 분류에 대해서도 이끼인지, 조류인지 등에 대한 논란이 있었으나 이번 게놈 분석으로 이끼라는 게 확인됐다며 시간 흐름에 따라 게놈이 크게 변했음에도 식물체 형태가 거의 변하지 않은 점은 새 연구 과제라고 밝혔다.',\n",
       " '연구팀은 그러나 타카키아가 과거 환경변화에 빠르게 적응해 살아남았지만 현재의 온난화와 서식지 감소 속도를 고려하면 앞으로 100년 이상 살아남기는 어려울 것으로 보인다고 우려했다.',\n",
       " '연구가 진행되는 동안 티베트고원의 타카키아 개체수는 매년 1.6%씩 감소했으며 서식지도 빠르게 줄어 금세기 말에는 타카키아에 적합한 서식지가 세계적으로 1천~1천500㎢밖에 남지 않을 것으로 예상됐다.',\n",
       " '연구팀은 타카키아의 멸종을 막기 위해 실험실에서 타카키아를 증식한 다음 티베트고원에 이식하는 시도를 하고 있으며 5년간 관찰 결과 이식된 식물 일부가 생존하고 번성하는 것으로 나타났다고 밝혔다.',\n",
       " '레스키 교수는 \"인간이 진화 정점에 있다고 생각하지만 공룡도 왔다가 사라진 것처럼 인간도 사라질 수 있다\"며 \"공룡의 등장과 멸종, 인간의 등장을 지켜본 타카키아로부터 회복력과 멸종에 대해 무언가를 배울 수 있을 것\"이라고 말했다.']"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import kss\n",
    "text = kss.split_sentences(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['BNK경남은행 간부의 562억원 횡령 사건이 채 잊히기도 전인데 또 다른 금융사고가 연발하고 있다.',\n",
       "  '수법도 거래 기업의 미공개 정보로 주식을 거래해 부당이득을 취하고, 고객 몰래 계좌를 개설하는 등 눈앞의 이익을 위해 온갖 불법·편법 행위가 동원되고 있다.',\n",
       "  'KB국민은행의 직원들이 고객사의 미공개정보를 이용해 주식을 사고팔아 127억원의 부당이득을 챙긴 사실이 최근 금융당국에 적발됐다.',\n",
       "  '이들은 2021년 1월부터 올해 4월까지 61개 상장사의 증권 업무를 대행하며 알게 된 무상증자 규모와 일정을 주식 매수에 이용했다.',\n",
       "  '정보 공개 전 미리 주식을 사뒀다가 공시 뒤 주가가 오르면 팔았다.',\n",
       "  '무상증자를 하게 되면 기업재무구조가 건전한 것으로 풀이돼 주가에는 호재로 작용한다.',\n",
       "  '이런 방식으로 66억원 정도를 챙겼고, 일부는 다른 부서의 동료나 가족 등에게도 정보를 전달했다.',\n",
       "  '이 과정에서도 61억원 상당의 부당 이득이 발생했다.',\n",
       "  '대구은행 일부 지점 직원 수십명은 평가 실적을 올리기 위해 지난해 1000여건이 넘는 고객 문서를 위조해 증권 계좌를 개설한 것으로 파악됐다.',\n",
       "  '이 직원들은 내점한 고객을 상대로 증권사 연계 계좌를 만들어 달라고 요청한 뒤 해당 계좌 신청서를 복사해 고객의 동의 없이 같은 증권사의 계좌를 하나 더 만들었다.',\n",
       "  'A증권사 위탁 계좌 개설 신청서를 받고, 같은 신청서를 복사해 A증권사 해외선물계좌까지 개설하는 방식이다.',\n",
       "  '최근 한 고객이 동의하지 않은 계좌가 개설됐다는 사실을 알게 돼 대구은행에 민원을 제기하면서 직원들의 비리가 드러났다.',\n",
       "  '대구은행은 문제를 인지하고도 금감원에 이 사실을 보고하지 않았고, 영업점들에 공문을 보내 불건전 영업행위를 예방하라고 안내하는 데 그쳤다.',\n",
       "  '금융실명제법 위반, 사문서 위조 등에 해당할 수 있는 범죄행위를 대수롭지 않게 넘기는 안일함이 혀를 차게 한다.',\n",
       "  '국내 은행은 땅 짚고 헤엄치기식 이자장사로 평균 1억원대 고연봉을 누리는 직종이다.',\n",
       "  '시중은행은 미국발 고금리에 편승해 거둬들인 막대한 예대마진으로 최근 수년간 성과급 잔치를 벌여 국민의 눈총을 받았다.',\n",
       "  '국민의 재산으로 손쉽게 수익을 올리는 직종이라면 누구보다 엄격한 도덕적 기준을 세워도 모자랄 판에 ‘내 몫을 더 챙기겠다’며 이기적 탐욕을 부리고 있으니 말문이 막힌다.',\n",
       "  '자체 내부통제가 안 된다면 현행 솜방망이 처벌 수위를 높이는 수밖에 없다.',\n",
       "  '주요 동기가 경제적 이익인 만큼 벌금이나 과징금, 양형 부과 수준을 크게 높여 법 무서운 줄 알도록 해야 한다.',\n",
       "  '주요 선진국에서 이미 도입한 불공정거래 범죄자에 대한 자본시장 거래제한제도도 적극 검토할 필요가 있다.']]"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src = []\n",
    "src.append(text)\n",
    "src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B N K 경 남 은 행   간 부 의   5 6 2 억 원   횡 령   사 건 이   채   잊 히 기 도   전 인 데   또   다 른   금 융 사 고 가   연 발 하 고   있 다 .',\n",
       " '수 법 도   거 래   기 업 의   미 공 개   정 보 로   주 식 을   거 래 해   부 당 이 득 을   취 하 고 ,   고 객   몰 래   계 좌 를   개 설 하 는   등   눈 앞 의   이 익 을   위 해   온 갖   불 법 · 편 법   행 위 가   동 원 되 고   있 다 .',\n",
       " 'K B 국 민 은 행 의   직 원 들 이   고 객 사 의   미 공 개 정 보 를   이 용 해   주 식 을   사 고 팔 아   1 2 7 억 원 의   부 당 이 득 을   챙 긴   사 실 이   최 근   금 융 당 국 에   적 발 됐 다 .',\n",
       " '이 들 은   2 0 2 1 년   1 월 부 터   올 해   4 월 까 지   6 1 개   상 장 사 의   증 권   업 무 를   대 행 하 며   알 게   된   무 상 증 자   규 모 와   일 정 을   주 식   매 수 에   이 용 했 다 .',\n",
       " '정 보   공 개   전   미 리   주 식 을   사 뒀 다 가   공 시   뒤   주 가 가   오 르 면   팔 았 다 .',\n",
       " '무 상 증 자 를   하 게   되 면   기 업 재 무 구 조 가   건 전 한   것 으 로   풀 이 돼   주 가 에 는   호 재 로   작 용 한 다 .',\n",
       " '이 런   방 식 으 로   6 6 억 원   정 도 를   챙 겼 고 ,   일 부 는   다 른   부 서 의   동 료 나   가 족   등 에 게 도   정 보 를   전 달 했 다 .',\n",
       " '이   과 정 에 서 도   6 1 억 원   상 당 의   부 당   이 득 이   발 생 했 다 .',\n",
       " '대 구 은 행   일 부   지 점   직 원   수 십 명 은   평 가   실 적 을   올 리 기   위 해   지 난 해   1 0 0 0 여 건 이   넘 는   고 객   문 서 를   위 조 해   증 권   계 좌 를   개 설 한   것 으 로   파 악 됐 다 .',\n",
       " '이   직 원 들 은   내 점 한   고 객 을   상 대 로   증 권 사   연 계   계 좌 를   만 들 어   달 라 고   요 청 한   뒤   해 당   계 좌   신 청 서 를   복 사 해   고 객 의   동 의   없 이   같 은   증 권 사 의   계 좌 를   하 나   더   만 들 었 다 .',\n",
       " 'A 증 권 사   위 탁   계 좌   개 설   신 청 서 를   받 고 ,   같 은   신 청 서 를   복 사 해   A 증 권 사   해 외 선 물 계 좌 까 지   개 설 하 는   방 식 이 다 .',\n",
       " '최 근   한   고 객 이   동 의 하 지   않 은   계 좌 가   개 설 됐 다 는   사 실 을   알 게   돼   대 구 은 행 에   민 원 을   제 기 하 면 서   직 원 들 의   비 리 가   드 러 났 다 .',\n",
       " '대 구 은 행 은   문 제 를   인 지 하 고 도   금 감 원 에   이   사 실 을   보 고 하 지   않 았 고 ,   영 업 점 들 에   공 문 을   보 내   불 건 전   영 업 행 위 를   예 방 하 라 고   안 내 하 는   데   그 쳤 다 .',\n",
       " '금 융 실 명 제 법   위 반 ,   사 문 서   위 조   등 에   해 당 할   수   있 는   범 죄 행 위 를   대 수 롭 지   않 게   넘 기 는   안 일 함 이   혀 를   차 게   한 다 .',\n",
       " '국 내   은 행 은   땅   짚 고   헤 엄 치 기 식   이 자 장 사 로   평 균   1 억 원 대   고 연 봉 을   누 리 는   직 종 이 다 .',\n",
       " '시 중 은 행 은   미 국 발   고 금 리 에   편 승 해   거 둬 들 인   막 대 한   예 대 마 진 으 로   최 근   수 년 간   성 과 급   잔 치 를   벌 여   국 민 의   눈 총 을   받 았 다 .',\n",
       " '국 민 의   재 산 으 로   손 쉽 게   수 익 을   올 리 는   직 종 이 라 면   누 구 보 다   엄 격 한   도 덕 적   기 준 을   세 워 도   모 자 랄   판 에   ‘ 내   몫 을   더   챙 기 겠 다 ’ 며   이 기 적   탐 욕 을   부 리 고   있 으 니   말 문 이   막 힌 다 .',\n",
       " '자 체   내 부 통 제 가   안   된 다 면   현 행   솜 방 망 이   처 벌   수 위 를   높 이 는   수 밖 에   없 다 .',\n",
       " '주 요   동 기 가   경 제 적   이 익 인   만 큼   벌 금 이 나   과 징 금 ,   양 형   부 과   수 준 을   크 게   높 여   법   무 서 운   줄   알 도 록   해 야   한 다 .',\n",
       " '주 요   선 진 국 에 서   이 미   도 입 한   불 공 정 거 래   범 죄 자 에   대 한   자 본 시 장   거 래 제 한 제 도 도   적 극   검 토 할   필 요 가   있 다 .']"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_src_txt = [' '.join(s) for s in text]\n",
    "original_src_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxs = [i for i, s in enumerate(text) if (len(s) > 1)]\n",
    "idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BNK경남은행 간부의 562억원 횡령 사건이 채 잊히기도 전인데 또 다른 금융사고가 연발하고 있다.',\n",
       " '수법도 거래 기업의 미공개 정보로 주식을 거래해 부당이득을 취하고, 고객 몰래 계좌를 개설하는 등 눈앞의 이익을 위해 온갖 불법·편법 행위가 동원되고 있다.',\n",
       " 'KB국민은행의 직원들이 고객사의 미공개정보를 이용해 주식을 사고팔아 127억원의 부당이득을 챙긴 사실이 최근 금융당국에 적발됐다.',\n",
       " '이들은 2021년 1월부터 올해 4월까지 61개 상장사의 증권 업무를 대행하며 알게 된 무상증자 규모와 일정을 주식 매수에 이용했다.',\n",
       " '정보 공개 전 미리 주식을 사뒀다가 공시 뒤 주가가 오르면 팔았다.',\n",
       " '무상증자를 하게 되면 기업재무구조가 건전한 것으로 풀이돼 주가에는 호재로 작용한다.',\n",
       " '이런 방식으로 66억원 정도를 챙겼고, 일부는 다른 부서의 동료나 가족 등에게도 정보를 전달했다.',\n",
       " '이 과정에서도 61억원 상당의 부당 이득이 발생했다.',\n",
       " '대구은행 일부 지점 직원 수십명은 평가 실적을 올리기 위해 지난해 1000여건이 넘는 고객 문서를 위조해 증권 계좌를 개설한 것으로 파악됐다.',\n",
       " '이 직원들은 내점한 고객을 상대로 증권사 연계 계좌를 만들어 달라고 요청한 뒤 해당 계좌 신청서를 복사해 고객의 동의 없이 같은 증권사의 계좌를 하나 더 만들었다.',\n",
       " 'A증권사 위탁 계좌 개설 신청서를 받고, 같은 신청서를 복사해 A증권사 해외선물계좌까지 개설하는 방식이다.',\n",
       " '최근 한 고객이 동의하지 않은 계좌가 개설됐다는 사실을 알게 돼 대구은행에 민원을 제기하면서 직원들의 비리가 드러났다.',\n",
       " '대구은행은 문제를 인지하고도 금감원에 이 사실을 보고하지 않았고, 영업점들에 공문을 보내 불건전 영업행위를 예방하라고 안내하는 데 그쳤다.',\n",
       " '금융실명제법 위반, 사문서 위조 등에 해당할 수 있는 범죄행위를 대수롭지 않게 넘기는 안일함이 혀를 차게 한다.',\n",
       " '국내 은행은 땅 짚고 헤엄치기식 이자장사로 평균 1억원대 고연봉을 누리는 직종이다.',\n",
       " '시중은행은 미국발 고금리에 편승해 거둬들인 막대한 예대마진으로 최근 수년간 성과급 잔치를 벌여 국민의 눈총을 받았다.',\n",
       " '국민의 재산으로 손쉽게 수익을 올리는 직종이라면 누구보다 엄격한 도덕적 기준을 세워도 모자랄 판에 ‘내 몫을 더 챙기겠다’며 이기적 탐욕을 부리고 있으니 말문이 막힌다.',\n",
       " '자체 내부통제가 안 된다면 현행 솜방망이 처벌 수위를 높이는 수밖에 없다.',\n",
       " '주요 동기가 경제적 이익인 만큼 벌금이나 과징금, 양형 부과 수준을 크게 높여 법 무서운 줄 알도록 해야 한다.',\n",
       " '주요 선진국에서 이미 도입한 불공정거래 범죄자에 대한 자본시장 거래제한제도도 적극 검토할 필요가 있다.']"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [text[i][:2000] for i in idxs]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = text[:1000]\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'KoBertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = KoBertTokenizer.from_pretrained(\"monologg/kobert\", do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B N K 경 남 은 행   간 부 의   5 6 2 억 원   횡 령   사 건 이   채   잊 히 기 도   전 인 데   또   다 른   금 융 사 고 가   연 발 하 고   있 다 .',\n",
       " '수 법 도   거 래   기 업 의   미 공 개   정 보 로   주 식 을   거 래 해   부 당 이 득 을   취 하 고 ,   고 객   몰 래   계 좌 를   개 설 하 는   등   눈 앞 의   이 익 을   위 해   온 갖   불 법 · 편 법   행 위 가   동 원 되 고   있 다 .',\n",
       " 'K B 국 민 은 행 의   직 원 들 이   고 객 사 의   미 공 개 정 보 를   이 용 해   주 식 을   사 고 팔 아   1 2 7 억 원 의   부 당 이 득 을   챙 긴   사 실 이   최 근   금 융 당 국 에   적 발 됐 다 .',\n",
       " '이 들 은   2 0 2 1 년   1 월 부 터   올 해   4 월 까 지   6 1 개   상 장 사 의   증 권   업 무 를   대 행 하 며   알 게   된   무 상 증 자   규 모 와   일 정 을   주 식   매 수 에   이 용 했 다 .',\n",
       " '정 보   공 개   전   미 리   주 식 을   사 뒀 다 가   공 시   뒤   주 가 가   오 르 면   팔 았 다 .',\n",
       " '무 상 증 자 를   하 게   되 면   기 업 재 무 구 조 가   건 전 한   것 으 로   풀 이 돼   주 가 에 는   호 재 로   작 용 한 다 .',\n",
       " '이 런   방 식 으 로   6 6 억 원   정 도 를   챙 겼 고 ,   일 부 는   다 른   부 서 의   동 료 나   가 족   등 에 게 도   정 보 를   전 달 했 다 .',\n",
       " '이   과 정 에 서 도   6 1 억 원   상 당 의   부 당   이 득 이   발 생 했 다 .',\n",
       " '대 구 은 행   일 부   지 점   직 원   수 십 명 은   평 가   실 적 을   올 리 기   위 해   지 난 해   1 0 0 0 여 건 이   넘 는   고 객   문 서 를   위 조 해   증 권   계 좌 를   개 설 한   것 으 로   파 악 됐 다 .',\n",
       " '이   직 원 들 은   내 점 한   고 객 을   상 대 로   증 권 사   연 계   계 좌 를   만 들 어   달 라 고   요 청 한   뒤   해 당   계 좌   신 청 서 를   복 사 해   고 객 의   동 의   없 이   같 은   증 권 사 의   계 좌 를   하 나   더   만 들 었 다 .',\n",
       " 'A 증 권 사   위 탁   계 좌   개 설   신 청 서 를   받 고 ,   같 은   신 청 서 를   복 사 해   A 증 권 사   해 외 선 물 계 좌 까 지   개 설 하 는   방 식 이 다 .',\n",
       " '최 근   한   고 객 이   동 의 하 지   않 은   계 좌 가   개 설 됐 다 는   사 실 을   알 게   돼   대 구 은 행 에   민 원 을   제 기 하 면 서   직 원 들 의   비 리 가   드 러 났 다 .',\n",
       " '대 구 은 행 은   문 제 를   인 지 하 고 도   금 감 원 에   이   사 실 을   보 고 하 지   않 았 고 ,   영 업 점 들 에   공 문 을   보 내   불 건 전   영 업 행 위 를   예 방 하 라 고   안 내 하 는   데   그 쳤 다 .',\n",
       " '금 융 실 명 제 법   위 반 ,   사 문 서   위 조   등 에   해 당 할   수   있 는   범 죄 행 위 를   대 수 롭 지   않 게   넘 기 는   안 일 함 이   혀 를   차 게   한 다 .',\n",
       " '국 내   은 행 은   땅   짚 고   헤 엄 치 기 식   이 자 장 사 로   평 균   1 억 원 대   고 연 봉 을   누 리 는   직 종 이 다 .',\n",
       " '시 중 은 행 은   미 국 발   고 금 리 에   편 승 해   거 둬 들 인   막 대 한   예 대 마 진 으 로   최 근   수 년 간   성 과 급   잔 치 를   벌 여   국 민 의   눈 총 을   받 았 다 .',\n",
       " '국 민 의   재 산 으 로   손 쉽 게   수 익 을   올 리 는   직 종 이 라 면   누 구 보 다   엄 격 한   도 덕 적   기 준 을   세 워 도   모 자 랄   판 에   ‘ 내   몫 을   더   챙 기 겠 다 ’ 며   이 기 적   탐 욕 을   부 리 고   있 으 니   말 문 이   막 힌 다 .',\n",
       " '자 체   내 부 통 제 가   안   된 다 면   현 행   솜 방 망 이   처 벌   수 위 를   높 이 는   수 밖 에   없 다 .',\n",
       " '주 요   동 기 가   경 제 적   이 익 인   만 큼   벌 금 이 나   과 징 금 ,   양 형   부 과   수 준 을   크 게   높 여   법   무 서 운   줄   알 도 록   해 야   한 다 .',\n",
       " '주 요   선 진 국 에 서   이 미   도 입 한   불 공 정 거 래   범 죄 자 에   대 한   자 본 시 장   거 래 제 한 제 도 도   적 극   검 토 할   필 요 가   있 다 .']"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_txt = [' '.join(sent) for sent in text]\n",
    "src_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'B N K 경 남 은 행   간 부 의   5 6 2 억 원   횡 령   사 건 이   채   잊 히 기 도   전 인 데   또   다 른   금 융 사 고 가   연 발 하 고   있 다 . [SEP] [CLS] 수 법 도   거 래   기 업 의   미 공 개   정 보 로   주 식 을   거 래 해   부 당 이 득 을   취 하 고 ,   고 객   몰 래   계 좌 를   개 설 하 는   등   눈 앞 의   이 익 을   위 해   온 갖   불 법 · 편 법   행 위 가   동 원 되 고   있 다 . [SEP] [CLS] K B 국 민 은 행 의   직 원 들 이   고 객 사 의   미 공 개 정 보 를   이 용 해   주 식 을   사 고 팔 아   1 2 7 억 원 의   부 당 이 득 을   챙 긴   사 실 이   최 근   금 융 당 국 에   적 발 됐 다 . [SEP] [CLS] 이 들 은   2 0 2 1 년   1 월 부 터   올 해   4 월 까 지   6 1 개   상 장 사 의   증 권   업 무 를   대 행 하 며   알 게   된   무 상 증 자   규 모 와   일 정 을   주 식   매 수 에   이 용 했 다 . [SEP] [CLS] 정 보   공 개   전   미 리   주 식 을   사 뒀 다 가   공 시   뒤   주 가 가   오 르 면   팔 았 다 . [SEP] [CLS] 무 상 증 자 를   하 게   되 면   기 업 재 무 구 조 가   건 전 한   것 으 로   풀 이 돼   주 가 에 는   호 재 로   작 용 한 다 . [SEP] [CLS] 이 런   방 식 으 로   6 6 억 원   정 도 를   챙 겼 고 ,   일 부 는   다 른   부 서 의   동 료 나   가 족   등 에 게 도   정 보 를   전 달 했 다 . [SEP] [CLS] 이   과 정 에 서 도   6 1 억 원   상 당 의   부 당   이 득 이   발 생 했 다 . [SEP] [CLS] 대 구 은 행   일 부   지 점   직 원   수 십 명 은   평 가   실 적 을   올 리 기   위 해   지 난 해   1 0 0 0 여 건 이   넘 는   고 객   문 서 를   위 조 해   증 권   계 좌 를   개 설 한   것 으 로   파 악 됐 다 . [SEP] [CLS] 이   직 원 들 은   내 점 한   고 객 을   상 대 로   증 권 사   연 계   계 좌 를   만 들 어   달 라 고   요 청 한   뒤   해 당   계 좌   신 청 서 를   복 사 해   고 객 의   동 의   없 이   같 은   증 권 사 의   계 좌 를   하 나   더   만 들 었 다 . [SEP] [CLS] A 증 권 사   위 탁   계 좌   개 설   신 청 서 를   받 고 ,   같 은   신 청 서 를   복 사 해   A 증 권 사   해 외 선 물 계 좌 까 지   개 설 하 는   방 식 이 다 . [SEP] [CLS] 최 근   한   고 객 이   동 의 하 지   않 은   계 좌 가   개 설 됐 다 는   사 실 을   알 게   돼   대 구 은 행 에   민 원 을   제 기 하 면 서   직 원 들 의   비 리 가   드 러 났 다 . [SEP] [CLS] 대 구 은 행 은   문 제 를   인 지 하 고 도   금 감 원 에   이   사 실 을   보 고 하 지   않 았 고 ,   영 업 점 들 에   공 문 을   보 내   불 건 전   영 업 행 위 를   예 방 하 라 고   안 내 하 는   데   그 쳤 다 . [SEP] [CLS] 금 융 실 명 제 법   위 반 ,   사 문 서   위 조   등 에   해 당 할   수   있 는   범 죄 행 위 를   대 수 롭 지   않 게   넘 기 는   안 일 함 이   혀 를   차 게   한 다 . [SEP] [CLS] 국 내   은 행 은   땅   짚 고   헤 엄 치 기 식   이 자 장 사 로   평 균   1 억 원 대   고 연 봉 을   누 리 는   직 종 이 다 . [SEP] [CLS] 시 중 은 행 은   미 국 발   고 금 리 에   편 승 해   거 둬 들 인   막 대 한   예 대 마 진 으 로   최 근   수 년 간   성 과 급   잔 치 를   벌 여   국 민 의   눈 총 을   받 았 다 . [SEP] [CLS] 국 민 의   재 산 으 로   손 쉽 게   수 익 을   올 리 는   직 종 이 라 면   누 구 보 다   엄 격 한   도 덕 적   기 준 을   세 워 도   모 자 랄   판 에   ‘ 내   몫 을   더   챙 기 겠 다 ’ 며   이 기 적   탐 욕 을   부 리 고   있 으 니   말 문 이   막 힌 다 . [SEP] [CLS] 자 체   내 부 통 제 가   안   된 다 면   현 행   솜 방 망 이   처 벌   수 위 를   높 이 는   수 밖 에   없 다 . [SEP] [CLS] 주 요   동 기 가   경 제 적   이 익 인   만 큼   벌 금 이 나   과 징 금 ,   양 형   부 과   수 준 을   크 게   높 여   법   무 서 운   줄   알 도 록   해 야   한 다 . [SEP] [CLS] 주 요   선 진 국 에 서   이 미   도 입 한   불 공 정 거 래   범 죄 자 에   대 한   자 본 시 장   거 래 제 한 제 도 도   적 극   검 토 할   필 요 가   있 다 .'"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ' [SEP] [CLS] '.join(src_txt)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁',\n",
       " 'b',\n",
       " '▁',\n",
       " 'n',\n",
       " '▁',\n",
       " 'k',\n",
       " '▁경',\n",
       " '▁남',\n",
       " '▁',\n",
       " '은',\n",
       " '▁행',\n",
       " '▁간',\n",
       " '▁부',\n",
       " '▁',\n",
       " '의',\n",
       " '▁5',\n",
       " '▁6',\n",
       " '▁2',\n",
       " '▁',\n",
       " '억',\n",
       " '▁원',\n",
       " '▁',\n",
       " '횡',\n",
       " '▁',\n",
       " '령',\n",
       " '▁사',\n",
       " '▁건',\n",
       " '▁이',\n",
       " '▁채',\n",
       " '▁',\n",
       " '잊',\n",
       " '▁',\n",
       " '히',\n",
       " '▁기',\n",
       " '▁',\n",
       " '도',\n",
       " '▁전',\n",
       " '▁인',\n",
       " '▁데',\n",
       " '▁또',\n",
       " '▁다',\n",
       " '▁',\n",
       " '른',\n",
       " '▁금',\n",
       " '▁',\n",
       " '융',\n",
       " '▁사',\n",
       " '▁고',\n",
       " '▁',\n",
       " '가',\n",
       " '▁연',\n",
       " '▁발',\n",
       " '▁하',\n",
       " '▁고',\n",
       " '▁있',\n",
       " '▁다',\n",
       " '▁',\n",
       " '.',\n",
       " '[SEP]',\n",
       " '[CLS]',\n",
       " '▁수',\n",
       " '▁법',\n",
       " '▁',\n",
       " '도',\n",
       " '▁거',\n",
       " '▁',\n",
       " '래',\n",
       " '▁기',\n",
       " '▁업',\n",
       " '▁',\n",
       " '의',\n",
       " '▁미',\n",
       " '▁공',\n",
       " '▁개',\n",
       " '▁정',\n",
       " '▁보',\n",
       " '▁',\n",
       " '로',\n",
       " '▁주',\n",
       " '▁식',\n",
       " '▁',\n",
       " '을',\n",
       " '▁거',\n",
       " '▁',\n",
       " '래',\n",
       " '▁해',\n",
       " '▁부',\n",
       " '▁당',\n",
       " '▁이',\n",
       " '▁',\n",
       " '득',\n",
       " '▁',\n",
       " '을',\n",
       " '▁취',\n",
       " '▁하',\n",
       " '▁고',\n",
       " '▁',\n",
       " ',',\n",
       " '▁고',\n",
       " '▁',\n",
       " '객',\n",
       " '▁몰',\n",
       " '▁',\n",
       " '래',\n",
       " '▁계',\n",
       " '▁좌',\n",
       " '▁',\n",
       " '를',\n",
       " '▁개',\n",
       " '▁설',\n",
       " '▁하',\n",
       " '▁',\n",
       " '는',\n",
       " '▁등',\n",
       " '▁눈',\n",
       " '▁앞',\n",
       " '▁',\n",
       " '의',\n",
       " '▁이',\n",
       " '▁익',\n",
       " '▁',\n",
       " '을',\n",
       " '▁위',\n",
       " '▁해',\n",
       " '▁온',\n",
       " '▁갖',\n",
       " '▁불',\n",
       " '▁법',\n",
       " '▁',\n",
       " '·',\n",
       " '▁편',\n",
       " '▁법',\n",
       " '▁행',\n",
       " '▁위',\n",
       " '▁',\n",
       " '가',\n",
       " '▁동',\n",
       " '▁원',\n",
       " '▁되',\n",
       " '▁고',\n",
       " '▁있',\n",
       " '▁다',\n",
       " '▁',\n",
       " '.',\n",
       " '[SEP]',\n",
       " '[CLS]',\n",
       " '▁',\n",
       " 'k',\n",
       " '▁',\n",
       " 'b',\n",
       " '▁국',\n",
       " '▁민',\n",
       " '▁',\n",
       " '은',\n",
       " '▁행',\n",
       " '▁',\n",
       " '의',\n",
       " '▁직',\n",
       " '▁원',\n",
       " '▁들',\n",
       " '▁이',\n",
       " '▁고',\n",
       " '▁',\n",
       " '객',\n",
       " '▁사',\n",
       " '▁',\n",
       " '의',\n",
       " '▁미',\n",
       " '▁공',\n",
       " '▁개',\n",
       " '▁정',\n",
       " '▁보',\n",
       " '▁',\n",
       " '를',\n",
       " '▁이',\n",
       " '▁용',\n",
       " '▁해',\n",
       " '▁주',\n",
       " '▁식',\n",
       " '▁',\n",
       " '을',\n",
       " '▁사',\n",
       " '▁고',\n",
       " '▁팔',\n",
       " '▁아',\n",
       " '▁1',\n",
       " '▁2',\n",
       " '▁7',\n",
       " '▁',\n",
       " '억',\n",
       " '▁원',\n",
       " '▁',\n",
       " '의',\n",
       " '▁부',\n",
       " '▁당',\n",
       " '▁이',\n",
       " '▁',\n",
       " '득',\n",
       " '▁',\n",
       " '을',\n",
       " '▁',\n",
       " '챙',\n",
       " '▁긴',\n",
       " '▁사',\n",
       " '▁실',\n",
       " '▁이',\n",
       " '▁최',\n",
       " '▁근',\n",
       " '▁금',\n",
       " '▁',\n",
       " '융',\n",
       " '▁당',\n",
       " '▁국',\n",
       " '▁',\n",
       " '에',\n",
       " '▁적',\n",
       " '▁발',\n",
       " '▁',\n",
       " '됐',\n",
       " '▁다',\n",
       " '▁',\n",
       " '.',\n",
       " '[SEP]',\n",
       " '[CLS]',\n",
       " '▁이',\n",
       " '▁들',\n",
       " '▁',\n",
       " '은',\n",
       " '▁2',\n",
       " '▁0',\n",
       " '▁2',\n",
       " '▁1',\n",
       " '▁',\n",
       " '년',\n",
       " '▁1',\n",
       " '▁',\n",
       " '월',\n",
       " '▁부',\n",
       " '▁터',\n",
       " '▁올',\n",
       " '▁해',\n",
       " '▁4',\n",
       " '▁',\n",
       " '월',\n",
       " '▁',\n",
       " '까',\n",
       " '▁지',\n",
       " '▁6',\n",
       " '▁1',\n",
       " '▁개',\n",
       " '▁상',\n",
       " '▁장',\n",
       " '▁사',\n",
       " '▁',\n",
       " '의',\n",
       " '▁증',\n",
       " '▁권',\n",
       " '▁업',\n",
       " '▁무',\n",
       " '▁',\n",
       " '를',\n",
       " '▁대',\n",
       " '▁행',\n",
       " '▁하',\n",
       " '▁',\n",
       " '며',\n",
       " '▁알',\n",
       " '▁게',\n",
       " '▁된',\n",
       " '▁무',\n",
       " '▁상',\n",
       " '▁증',\n",
       " '▁자',\n",
       " '▁',\n",
       " '규',\n",
       " '▁모',\n",
       " '▁',\n",
       " '와',\n",
       " '▁일',\n",
       " '▁정',\n",
       " '▁',\n",
       " '을',\n",
       " '▁주',\n",
       " '▁식',\n",
       " '▁매',\n",
       " '▁수',\n",
       " '▁',\n",
       " '에',\n",
       " '▁이',\n",
       " '▁용',\n",
       " '▁했',\n",
       " '▁다',\n",
       " '▁',\n",
       " '.',\n",
       " '[SEP]',\n",
       " '[CLS]',\n",
       " '▁정',\n",
       " '▁보',\n",
       " '▁공',\n",
       " '▁개',\n",
       " '▁전',\n",
       " '▁미',\n",
       " '▁리',\n",
       " '▁주',\n",
       " '▁식',\n",
       " '▁',\n",
       " '을',\n",
       " '▁사',\n",
       " '▁',\n",
       " '뒀',\n",
       " '▁다',\n",
       " '▁',\n",
       " '가',\n",
       " '▁공',\n",
       " '▁시',\n",
       " '▁뒤',\n",
       " '▁주',\n",
       " '▁',\n",
       " '가',\n",
       " '▁',\n",
       " '가',\n",
       " '▁오',\n",
       " '▁',\n",
       " '르',\n",
       " '▁면',\n",
       " '▁팔',\n",
       " '▁',\n",
       " '았',\n",
       " '▁다',\n",
       " '▁',\n",
       " '.',\n",
       " '[SEP]',\n",
       " '[CLS]',\n",
       " '▁무',\n",
       " '▁상',\n",
       " '▁증',\n",
       " '▁자',\n",
       " '▁',\n",
       " '를',\n",
       " '▁하',\n",
       " '▁게',\n",
       " '▁되',\n",
       " '▁면',\n",
       " '▁기',\n",
       " '▁업',\n",
       " '▁재',\n",
       " '▁무',\n",
       " '▁구',\n",
       " '▁조',\n",
       " '▁',\n",
       " '가',\n",
       " '▁건',\n",
       " '▁전',\n",
       " '▁한',\n",
       " '▁것',\n",
       " '▁',\n",
       " '으',\n",
       " '▁',\n",
       " '로',\n",
       " '▁풀',\n",
       " '▁이',\n",
       " '▁돼',\n",
       " '▁주',\n",
       " '▁',\n",
       " '가',\n",
       " '▁',\n",
       " '에',\n",
       " '▁',\n",
       " '는',\n",
       " '▁호',\n",
       " '▁재',\n",
       " '▁',\n",
       " '로',\n",
       " '▁작',\n",
       " '▁용',\n",
       " '▁한',\n",
       " '▁다',\n",
       " '▁',\n",
       " '.',\n",
       " '[SEP]',\n",
       " '[CLS]',\n",
       " '▁이',\n",
       " '▁',\n",
       " '런',\n",
       " '▁방',\n",
       " '▁식',\n",
       " '▁',\n",
       " '으',\n",
       " '▁',\n",
       " '로',\n",
       " '▁6',\n",
       " '▁6',\n",
       " '▁',\n",
       " '억',\n",
       " '▁원',\n",
       " '▁정',\n",
       " '▁',\n",
       " '도',\n",
       " '▁',\n",
       " '를',\n",
       " '▁',\n",
       " '챙',\n",
       " '▁',\n",
       " '겼',\n",
       " '▁고',\n",
       " '▁',\n",
       " ',',\n",
       " '▁일',\n",
       " '▁부',\n",
       " '▁',\n",
       " '는',\n",
       " '▁다',\n",
       " '▁',\n",
       " '른',\n",
       " '▁부',\n",
       " '▁서',\n",
       " '▁',\n",
       " '의',\n",
       " '▁동',\n",
       " '▁',\n",
       " '료',\n",
       " '▁나',\n",
       " '▁',\n",
       " '가',\n",
       " '▁',\n",
       " '족',\n",
       " '▁등',\n",
       " '▁',\n",
       " '에',\n",
       " '▁게',\n",
       " '▁',\n",
       " '도',\n",
       " '▁정',\n",
       " '▁보',\n",
       " '▁',\n",
       " '를',\n",
       " '▁전',\n",
       " '▁달',\n",
       " '▁했',\n",
       " '▁다',\n",
       " '▁',\n",
       " '.',\n",
       " '[SEP]',\n",
       " '[CLS]',\n",
       " '▁이',\n",
       " '▁',\n",
       " '과',\n",
       " '▁정',\n",
       " '▁',\n",
       " '에',\n",
       " '▁서',\n",
       " '▁',\n",
       " '도',\n",
       " '▁6',\n",
       " '▁1',\n",
       " '▁',\n",
       " '억',\n",
       " '▁원',\n",
       " '▁상',\n",
       " '▁당',\n",
       " '▁',\n",
       " '의',\n",
       " '▁부',\n",
       " '▁당',\n",
       " '▁이',\n",
       " '▁',\n",
       " '득',\n",
       " '▁이',\n",
       " '▁발',\n",
       " '▁생',\n",
       " '▁했',\n",
       " '▁다',\n",
       " '▁',\n",
       " '.',\n",
       " '[SEP]',\n",
       " '[CLS]',\n",
       " '▁대',\n",
       " '▁구',\n",
       " '▁',\n",
       " '은',\n",
       " '▁행',\n",
       " '▁일',\n",
       " '▁부',\n",
       " '▁지',\n",
       " '▁점',\n",
       " '▁직',\n",
       " '▁원',\n",
       " '▁수',\n",
       " '▁',\n",
       " '십',\n",
       " '▁명',\n",
       " '▁',\n",
       " '은',\n",
       " '▁평',\n",
       " '▁',\n",
       " '가',\n",
       " '▁실',\n",
       " '▁적',\n",
       " '▁',\n",
       " '을',\n",
       " '▁올',\n",
       " '▁리',\n",
       " '▁기',\n",
       " '▁위',\n",
       " '▁해',\n",
       " '▁지',\n",
       " '▁난',\n",
       " '▁해',\n",
       " '▁1',\n",
       " '▁0',\n",
       " '▁0',\n",
       " '▁0',\n",
       " '▁여',\n",
       " '▁건',\n",
       " '▁이',\n",
       " '▁넘',\n",
       " '▁',\n",
       " '는',\n",
       " '▁고',\n",
       " '▁',\n",
       " '객',\n",
       " '▁문',\n",
       " '▁서',\n",
       " '▁',\n",
       " '를',\n",
       " '▁위',\n",
       " '▁조',\n",
       " '▁해',\n",
       " '▁증',\n",
       " '▁권',\n",
       " '▁계',\n",
       " '▁좌',\n",
       " '▁',\n",
       " '를',\n",
       " '▁개',\n",
       " '▁설',\n",
       " '▁한',\n",
       " '▁것',\n",
       " '▁',\n",
       " '으',\n",
       " '▁',\n",
       " '로',\n",
       " '▁파',\n",
       " '▁악',\n",
       " '▁',\n",
       " '됐',\n",
       " '▁다',\n",
       " '▁',\n",
       " '.',\n",
       " '[SEP]',\n",
       " '[CLS]',\n",
       " '▁이',\n",
       " '▁직',\n",
       " '▁원',\n",
       " '▁들',\n",
       " '▁',\n",
       " '은',\n",
       " '▁내',\n",
       " '▁점',\n",
       " '▁한',\n",
       " '▁고',\n",
       " '▁',\n",
       " '객',\n",
       " '▁',\n",
       " '을',\n",
       " '▁상',\n",
       " '▁대',\n",
       " '▁',\n",
       " '로',\n",
       " '▁증',\n",
       " '▁권',\n",
       " '▁사',\n",
       " '▁연',\n",
       " '▁계',\n",
       " '▁계',\n",
       " '▁좌',\n",
       " '▁',\n",
       " '를',\n",
       " '▁만',\n",
       " '▁들',\n",
       " '▁어',\n",
       " '▁달',\n",
       " '▁라',\n",
       " '▁고',\n",
       " '▁요',\n",
       " '▁청',\n",
       " '▁한',\n",
       " '▁뒤',\n",
       " '▁해',\n",
       " '▁당',\n",
       " '▁계',\n",
       " '▁좌',\n",
       " '▁신',\n",
       " '▁청',\n",
       " '▁서',\n",
       " '▁',\n",
       " '를',\n",
       " '▁복',\n",
       " '▁사',\n",
       " '▁해',\n",
       " '▁고',\n",
       " '▁',\n",
       " '객',\n",
       " '▁',\n",
       " '의',\n",
       " '▁동',\n",
       " '▁',\n",
       " '의',\n",
       " '▁없',\n",
       " '▁이',\n",
       " '▁같',\n",
       " '▁',\n",
       " '은',\n",
       " '▁증',\n",
       " '▁권',\n",
       " '▁사',\n",
       " '▁',\n",
       " '의',\n",
       " '▁계',\n",
       " '▁좌',\n",
       " '▁',\n",
       " '를',\n",
       " '▁하',\n",
       " '▁나',\n",
       " '▁더',\n",
       " '▁만',\n",
       " '▁들',\n",
       " '▁',\n",
       " '었',\n",
       " '▁다',\n",
       " '▁',\n",
       " '.',\n",
       " '[SEP]',\n",
       " '[CLS]',\n",
       " '▁',\n",
       " 'a',\n",
       " '▁증',\n",
       " '▁권',\n",
       " '▁사',\n",
       " '▁위',\n",
       " '▁',\n",
       " '탁',\n",
       " '▁계',\n",
       " '▁좌',\n",
       " '▁개',\n",
       " '▁설',\n",
       " '▁신',\n",
       " '▁청',\n",
       " '▁서',\n",
       " '▁',\n",
       " '를',\n",
       " '▁받',\n",
       " '▁고',\n",
       " '▁',\n",
       " ',',\n",
       " '▁같',\n",
       " '▁',\n",
       " '은',\n",
       " '▁신',\n",
       " '▁청',\n",
       " '▁서',\n",
       " '▁',\n",
       " '를',\n",
       " '▁복',\n",
       " '▁사',\n",
       " '▁해',\n",
       " '▁',\n",
       " 'a',\n",
       " '▁증',\n",
       " '▁권',\n",
       " '▁사',\n",
       " '▁해',\n",
       " '▁외',\n",
       " '▁선',\n",
       " '▁물',\n",
       " '▁계',\n",
       " '▁좌',\n",
       " '▁',\n",
       " '까',\n",
       " '▁지',\n",
       " '▁개',\n",
       " '▁설',\n",
       " '▁하',\n",
       " '▁',\n",
       " '는',\n",
       " '▁방',\n",
       " '▁식',\n",
       " '▁이',\n",
       " '▁다',\n",
       " '▁',\n",
       " '.',\n",
       " '[SEP]',\n",
       " '[CLS]',\n",
       " '▁최',\n",
       " '▁근',\n",
       " '▁한',\n",
       " '▁고',\n",
       " '▁',\n",
       " '객',\n",
       " '▁이',\n",
       " '▁동',\n",
       " '▁',\n",
       " '의',\n",
       " '▁하',\n",
       " '▁지',\n",
       " '▁않',\n",
       " '▁',\n",
       " '은',\n",
       " '▁계',\n",
       " '▁좌',\n",
       " '▁',\n",
       " '가',\n",
       " '▁개',\n",
       " '▁설',\n",
       " '▁',\n",
       " '됐',\n",
       " '▁다',\n",
       " '▁',\n",
       " '는',\n",
       " '▁사',\n",
       " '▁실',\n",
       " '▁',\n",
       " '을',\n",
       " '▁알',\n",
       " '▁게',\n",
       " '▁돼',\n",
       " '▁대',\n",
       " '▁구',\n",
       " '▁',\n",
       " '은',\n",
       " '▁행',\n",
       " '▁',\n",
       " '에',\n",
       " '▁민',\n",
       " '▁원',\n",
       " '▁',\n",
       " '을',\n",
       " '▁제',\n",
       " '▁기',\n",
       " '▁하',\n",
       " '▁면',\n",
       " '▁서',\n",
       " '▁직',\n",
       " '▁원',\n",
       " '▁들',\n",
       " '▁',\n",
       " '의',\n",
       " '▁비',\n",
       " '▁리',\n",
       " '▁',\n",
       " '가',\n",
       " '▁드',\n",
       " '▁',\n",
       " '러',\n",
       " '▁',\n",
       " '났',\n",
       " '▁다',\n",
       " '▁',\n",
       " '.',\n",
       " '[SEP]',\n",
       " '[CLS]',\n",
       " '▁대',\n",
       " '▁구',\n",
       " '▁',\n",
       " '은',\n",
       " '▁행',\n",
       " '▁',\n",
       " '은',\n",
       " '▁문',\n",
       " '▁제',\n",
       " '▁',\n",
       " '를',\n",
       " '▁인',\n",
       " '▁지',\n",
       " '▁하',\n",
       " '▁고',\n",
       " '▁',\n",
       " '도',\n",
       " '▁금',\n",
       " '▁감',\n",
       " '▁원',\n",
       " '▁',\n",
       " '에',\n",
       " '▁이',\n",
       " '▁사',\n",
       " '▁실',\n",
       " '▁',\n",
       " '을',\n",
       " '▁보',\n",
       " '▁고',\n",
       " '▁하',\n",
       " '▁지',\n",
       " '▁않',\n",
       " '▁',\n",
       " '았',\n",
       " '▁고',\n",
       " '▁',\n",
       " ',',\n",
       " '▁영',\n",
       " '▁업',\n",
       " '▁점',\n",
       " '▁들',\n",
       " '▁',\n",
       " '에',\n",
       " '▁공',\n",
       " '▁문',\n",
       " '▁',\n",
       " '을',\n",
       " '▁보',\n",
       " '▁내',\n",
       " '▁불',\n",
       " '▁건',\n",
       " '▁전',\n",
       " '▁영',\n",
       " '▁업',\n",
       " '▁행',\n",
       " '▁위',\n",
       " '▁',\n",
       " '를',\n",
       " '▁예',\n",
       " '▁방',\n",
       " '▁하',\n",
       " '▁라',\n",
       " '▁고',\n",
       " '▁안',\n",
       " '▁내',\n",
       " '▁하',\n",
       " '▁',\n",
       " '는',\n",
       " '▁데',\n",
       " '▁그',\n",
       " '▁',\n",
       " '쳤',\n",
       " '▁다',\n",
       " '▁',\n",
       " '.',\n",
       " '[SEP]',\n",
       " '[CLS]',\n",
       " '▁금',\n",
       " '▁',\n",
       " '융',\n",
       " '▁실',\n",
       " '▁명',\n",
       " '▁제',\n",
       " '▁법',\n",
       " '▁위',\n",
       " '▁반',\n",
       " '▁',\n",
       " ',',\n",
       " '▁사',\n",
       " '▁문',\n",
       " '▁서',\n",
       " '▁위',\n",
       " '▁조',\n",
       " '▁등',\n",
       " '▁',\n",
       " '에',\n",
       " '▁해',\n",
       " '▁당',\n",
       " '▁할',\n",
       " '▁수',\n",
       " '▁있',\n",
       " '▁',\n",
       " '는',\n",
       " '▁범',\n",
       " '▁죄',\n",
       " '▁행',\n",
       " '▁위',\n",
       " '▁',\n",
       " '를',\n",
       " '▁대',\n",
       " '▁수',\n",
       " '▁',\n",
       " '롭',\n",
       " '▁지',\n",
       " '▁않',\n",
       " '▁게',\n",
       " '▁넘',\n",
       " '▁기',\n",
       " '▁',\n",
       " '는',\n",
       " '▁안',\n",
       " '▁일',\n",
       " '▁함',\n",
       " '▁이',\n",
       " '▁',\n",
       " '혀',\n",
       " '▁',\n",
       " '를',\n",
       " '▁차',\n",
       " '▁게',\n",
       " '▁한',\n",
       " '▁다',\n",
       " '▁',\n",
       " '.',\n",
       " '[SEP]',\n",
       " '[CLS]',\n",
       " '▁국',\n",
       " '▁내',\n",
       " '▁',\n",
       " '은',\n",
       " '▁행',\n",
       " '▁',\n",
       " '은',\n",
       " '▁땅',\n",
       " '▁',\n",
       " '짚',\n",
       " '▁고',\n",
       " '▁헤',\n",
       " '▁엄',\n",
       " '▁치',\n",
       " '▁기',\n",
       " '▁식',\n",
       " '▁이',\n",
       " '▁자',\n",
       " '▁장',\n",
       " '▁사',\n",
       " '▁',\n",
       " '로',\n",
       " '▁평',\n",
       " '▁',\n",
       " '균',\n",
       " '▁1',\n",
       " '▁',\n",
       " '억',\n",
       " '▁원',\n",
       " '▁대',\n",
       " '▁고',\n",
       " '▁연',\n",
       " '▁봉',\n",
       " '▁',\n",
       " '을',\n",
       " '▁누',\n",
       " '▁리',\n",
       " '▁',\n",
       " '는',\n",
       " '▁직',\n",
       " '▁종',\n",
       " '▁이',\n",
       " '▁다',\n",
       " '▁',\n",
       " '.',\n",
       " '[SEP]',\n",
       " '[CLS]',\n",
       " '▁시',\n",
       " '▁중',\n",
       " '▁',\n",
       " '은',\n",
       " '▁행',\n",
       " '▁',\n",
       " '은',\n",
       " '▁미',\n",
       " '▁국',\n",
       " '▁발',\n",
       " '▁고',\n",
       " '▁금',\n",
       " '▁리',\n",
       " '▁',\n",
       " '에',\n",
       " '▁편',\n",
       " '▁승',\n",
       " '▁해',\n",
       " '▁거',\n",
       " '▁',\n",
       " '둬',\n",
       " '▁들',\n",
       " '▁인',\n",
       " '▁막',\n",
       " '▁대',\n",
       " '▁한',\n",
       " '▁예',\n",
       " '▁대',\n",
       " '▁마',\n",
       " '▁진',\n",
       " '▁',\n",
       " '으',\n",
       " '▁',\n",
       " '로',\n",
       " '▁최',\n",
       " '▁근',\n",
       " '▁수',\n",
       " '▁',\n",
       " '년',\n",
       " '▁간',\n",
       " '▁성',\n",
       " '▁',\n",
       " '과',\n",
       " '▁급',\n",
       " '▁잔',\n",
       " '▁치',\n",
       " '▁',\n",
       " '를',\n",
       " '▁벌',\n",
       " '▁여',\n",
       " '▁국',\n",
       " '▁민',\n",
       " '▁',\n",
       " '의',\n",
       " '▁눈',\n",
       " '▁총',\n",
       " ...]"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_subtokens = tokenizer.tokenize(text)\n",
    "src_subtokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522\n",
      "[[2, 522, 3], [2, 2718, 3], [2, 3524, 3], [2, 631, 3], [2, 3332, 3], [2, 4984, 3], [2, 1541, 3], [2, 2929, 3], [2, 3], [2, 517, 40, 3], [2, 3647, 3], [2, 4213, 3], [2, 3376, 3], [2, 1258, 3], [2, 3886, 3], [2, 631, 3], [2, 2554, 3], [2, 3], [2, 517, 6115, 3], [2, 4360, 3], [2, 5112, 3], [2, 2856, 3], [2, 3], [2, 517, 5859, 3], [2, 3], [2, 517, 6079, 3], [2, 3], [2, 517, 7930, 3], [2, 1725, 3], [2, 4955, 3], [2, 5136, 3], [2, 953, 3], [2, 3], [2, 517, 6896, 3], [2, 3996, 3], [2, 3612, 3], [2, 4924, 3], [2, 3], [2, 517, 6197, 3], [2, 605, 3], [2, 3], [2, 517, 6858, 3], [2, 3], [2, 517, 5712, 3], [2, 3], [2, 517, 7088, 3], [2, 2643, 3], [2, 3093, 3], [2, 3437, 3], [2, 3], [2, 517, 7996, 3], [2, 1958, 3], [2, 1875, 3], [2, 3206, 3], [2, 3], [2, 517, 7673, 3], [2, 2333, 3], [2, 4773, 3], [2, 993, 3], [2, 3533, 3], [2, 3], [2, 517, 7095, 3], [2, 3647, 3], [2, 1369, 3], [2, 3], [2, 517, 5859, 3], [2, 5049, 3], [2, 3969, 3], [2, 4360, 3], [2, 5022, 3], [2, 1763, 3], [2, 993, 3], [2, 3854, 3], [2, 3], [2, 517, 5760, 3], [2, 4297, 3], [2, 1115, 3], [2, 3437, 3], [2, 1406, 3], [2, 5112, 3], [2, 3], [2, 517, 6896, 3], [2, 3], [2, 517, 5760, 3], [2, 2643, 3], [2, 3093, 3], [2, 1409, 3], [2, 4297, 3], [2, 2086, 3], [2, 4977, 3], [2, 905, 3], [2, 3647, 3], [2, 1875, 3], [2, 3], [2, 517, 5760, 3], [2, 3332, 3], [2, 1115, 3], [2, 937, 3], [2, 3], [2, 517, 5468, 3], [2, 3], [2, 517, 5330, 3], [2, 1370, 3], [2, 3], [2, 517, 6989, 3], [2, 1562, 3], [2, 3], [2, 517, 54, 3], [2, 3, 3], [2, 2, 3], [2, 1725, 3], [2, 3803, 3], [2, 3], [2, 517, 7753, 3], [2, 1875, 3], [2, 3647, 3], [2, 2423, 3], [2, 3], [2, 517, 6113, 3], [2, 4681, 3], [2, 1633, 3], [2, 3], [2, 517, 6017, 3], [2, 3], [2, 517, 7753, 3], [2, 1884, 3], [2, 2929, 3], [2, 4693, 3], [2, 1103, 3], [2, 2872, 3], [2, 3], [2, 517, 6983, 3], [2, 4257, 3], [2, 1132, 3], [2, 2718, 3], [2, 3498, 3], [2, 1773, 3], [2, 2573, 3], [2, 2318, 3], [2, 1633, 3], [2, 5037, 3], [2, 3647, 3], [2, 3], [2, 517, 7553, 3], [2, 1103, 3], [2, 2872, 3], [2, 4797, 3], [2, 3], [2, 517, 7086, 3], [2, 529, 3], [2, 526, 3], [2, 3803, 3], [2, 3], [2, 517, 5468, 3], [2, 4948, 3], [2, 3990, 3], [2, 3], [2, 517, 5696, 3], [2, 520, 3], [2, 3], [2, 517, 6596, 3], [2, 520, 3], [2, 522, 3], [2, 705, 3], [2, 3], [2, 517, 389, 3], [2, 3], [2, 517, 420, 3], [2, 3], [2, 517, 420, 3], [2, 3], [2, 517, 40, 3], [2, 3], [2, 517, 6896, 3], [2, 2718, 3], [2, 3], [2, 517, 7673, 3], [2, 2333, 3], [2, 4773, 3], [2, 993, 3], [2, 3533, 3], [2, 1815, 3], [2, 3], [2, 517, 6896, 3], [2, 2573, 3], [2, 3], [2, 517, 5760, 3], [2, 5112, 3], [2, 2732, 3], [2, 3006, 3], [2, 2135, 3], [2, 3758, 3], [2, 4698, 3], [2, 4635, 3], [2, 4693, 3], [2, 3093, 3], [2, 3647, 3], [2, 1369, 3], [2, 3], [2, 517, 7095, 3], [2, 3], [2, 517, 388, 3], [2, 3], [2, 517, 425, 3], [2, 3], [2, 517, 367, 3], [2, 2468, 3], [2, 2732, 3], [2, 937, 3], [2, 3], [2, 517, 5468, 3], [2, 3574, 3], [2, 4012, 3], [2, 3996, 3], [2, 3], [2, 517, 7074, 3], [2, 3], [2, 517, 6079, 3], [2, 1986, 3], [2, 3498, 3], [2, 2554, 3], [2, 3], [2, 517, 6115, 3], [2, 4360, 3], [2, 5112, 3], [2, 4777, 3], [2, 2781, 3], [2, 3], [2, 517, 7088, 3], [2, 3], [2, 517, 5330, 3], [2, 3], [2, 517, 7248, 3], [2, 4297, 3], [2, 1931, 3], [2, 5049, 3], [2, 1258, 3], [2, 5176, 3], [2, 2339, 3], [2, 5112, 3], [2, 3], [2, 517, 6896, 3], [2, 2718, 3], [2, 2643, 3], [2, 3093, 3], [2, 1409, 3], [2, 3], [2, 517, 7088, 3], [2, 1931, 3], [2, 3], [2, 517, 7572, 3], [2, 2554, 3], [2, 3], [2, 517, 6113, 3], [2, 921, 3], [2, 4360, 3], [2, 5112, 3], [2, 4924, 3], [2, 4297, 3], [2, 3], [2, 517, 5760, 3], [2, 2086, 3], [2, 4977, 3], [2, 3], [2, 517, 5330, 3], [2, 3], [2, 517, 5765, 3], [2, 2781, 3], [2, 3647, 3], [2, 4688, 3], [2, 905, 3], [2, 3], [2, 517, 7074, 3], [2, 3], [2, 517, 6079, 3], [2, 1370, 3], [2, 4698, 3], [2, 3], [2, 517, 5671, 3], [2, 1562, 3], [2, 993, 3], [2, 3], [2, 517, 6301, 3], [2, 3], [2, 517, 7919, 3], [2, 1562, 3], [2, 3], [2, 517, 54, 3], [2, 3, 3], [2, 2, 3], [2, 3], [2, 517, 7673, 3], [2, 2333, 3], [2, 4773, 3], [2, 993, 3], [2, 3533, 3], [2, 3], [2, 517, 6870, 3], [2, 3606, 3], [2, 4067, 3], [2, 2338, 3], [2, 3], [2, 517, 6896, 3], [2, 2718, 3], [2, 589, 3], [2, 3], [2, 517, 6858, 3], [2, 627, 3], [2, 4471, 3], [2, 1931, 3], [2, 3], [2, 517, 5712, 3], [2, 3647, 3], [2, 1370, 3], [2, 2643, 3], [2, 3093, 3], [2, 3437, 3], [2, 4698, 3], [2, 4635, 3], [2, 4693, 3], [2, 3093, 3], [2, 3], [2, 517, 5760, 3], [2, 3930, 3], [2, 993, 3], [2, 3], [2, 517, 5757, 3], [2, 1900, 3], [2, 921, 3], [2, 3886, 3], [2, 1875, 3], [2, 3], [2, 517, 5760, 3], [2, 3647, 3], [2, 1369, 3], [2, 3], [2, 517, 6079, 3], [2, 3], [2, 517, 7996, 3], [2, 1958, 3], [2, 1875, 3], [2, 3206, 3], [2, 605, 3], [2, 4471, 3], [2, 3], [2, 517, 423, 3], [2, 993, 3], [2, 4297, 3], [2, 1633, 3], [2, 3], [2, 517, 6983, 3], [2, 3803, 3], [2, 2408, 3], [2, 3], [2, 517, 46, 3], [2, 2149, 3], [2, 1132, 3], [2, 1815, 3], [2, 3803, 3], [2, 2423, 3], [2, 4297, 3], [2, 3322, 3], [2, 3], [2, 517, 6896, 3], [2, 2718, 3], [2, 1931, 3], [2, 2417, 3], [2, 2872, 3], [2, 3854, 3], [2, 1562, 3], [2, 3], [2, 517, 54, 3], [2, 3, 3], [2, 2, 3], [2, 3332, 3], [2, 1115, 3], [2, 4797, 3], [2, 3], [2, 517, 7086, 3], [2, 3], [2, 517, 7673, 3], [2, 2333, 3], [2, 4773, 3], [2, 993, 3], [2, 3533, 3], [2, 3], [2, 517, 7095, 3], [2, 4698, 3], [2, 4635, 3], [2, 4693, 3], [2, 3093, 3], [2, 2718, 3], [2, 3006, 3], [2, 4297, 3], [2, 3], [2, 517, 6116, 3], [2, 529, 3], [2, 526, 3], [2, 3], [2, 517, 5712, 3], [2, 777, 3], [2, 529, 3], [2, 624, 3], [2, 4402, 3], [2, 3], [2, 517, 6078, 3], [2, 2267, 3], [2, 2120, 3], [2, 4998, 3], [2, 3], [2, 517, 6541, 3], [2, 3], [2, 517, 7761, 3], [2, 3], [2, 517, 7088, 3], [2, 2872, 3], [2, 4384, 3], [2, 4924, 3], [2, 993, 3], [2, 2718, 3], [2, 3006, 3], [2, 4297, 3], [2, 3], [2, 517, 6116, 3], [2, 4162, 3], [2, 2573, 3], [2, 5017, 3], [2, 1562, 3], [2, 3], [2, 517, 54, 3], [2, 3, 3], [2, 2, 3], [2, 4698, 3], [2, 4635, 3], [2, 4693, 3], [2, 3093, 3], [2, 3], [2, 517, 7095, 3], [2, 3], [2, 517, 388, 3], [2, 3], [2, 517, 425, 3], [2, 3], [2, 517, 367, 3], [2, 3374, 3], [2, 1258, 3], [2, 2718, 3], [2, 3358, 3], [2, 3], [2, 517, 7088, 3], [2, 2468, 3], [2, 2732, 3], [2, 4924, 3], [2, 993, 3], [2, 1258, 3], [2, 5176, 3], [2, 2339, 3], [2, 5112, 3], [2, 3], [2, 517, 5330, 3], [2, 4698, 3], [2, 4635, 3], [2, 4693, 3], [2, 3093, 3], [2, 3], [2, 517, 6896, 3], [2, 3220, 3], [2, 3], [2, 517, 5971, 3], [2, 3376, 3], [2, 5032, 3], [2, 3], [2, 517, 7088, 3], [2, 2149, 3], [2, 4617, 3], [2, 3], [2, 517, 5760, 3], [2, 4297, 3], [2, 3332, 3], [2, 1115, 3], [2, 5017, 3], [2, 1562, 3], [2, 3], [2, 517, 54, 3], [2, 3, 3], [2, 2, 3], [2, 1884, 3], [2, 2929, 3], [2, 4693, 3], [2, 1103, 3], [2, 2872, 3], [2, 3], [2, 517, 5760, 3], [2, 4297, 3], [2, 773, 3], [2, 2339, 3], [2, 1741, 3], [2, 3], [2, 517, 7074, 3], [2, 3], [2, 517, 6079, 3], [2, 3], [2, 517, 7996, 3], [2, 1958, 3], [2, 1875, 3], [2, 3206, 3], [2, 2640, 3], [2, 3], [2, 517, 6172, 3], [2, 3647, 3], [2, 3], [2, 517, 6620, 3], [2, 3093, 3], [2, 3439, 3], [2, 3], [2, 517, 6021, 3], [2, 3], [2, 517, 7088, 3], [2, 1844, 3], [2, 3], [2, 517, 5760, 3], [2, 4698, 3], [2, 4635, 3], [2, 4693, 3], [2, 3093, 3], [2, 3], [2, 517, 5330, 3], [2, 1815, 3], [2, 3954, 3], [2, 4955, 3], [2, 4297, 3], [2, 529, 3], [2, 3], [2, 517, 6858, 3], [2, 3], [2, 517, 5712, 3], [2, 3647, 3], [2, 4297, 3], [2, 1406, 3], [2, 2959, 3], [2, 4074, 3], [2, 3647, 3], [2, 3], [2, 517, 6885, 3], [2, 993, 3], [2, 3], [2, 517, 46, 3], [2, 3647, 3], [2, 3], [2, 517, 6040, 3], [2, 1250, 3], [2, 931, 3], [2, 4955, 3], [2, 5136, 3], [2, 953, 3], [2, 2339, 3], [2, 5112, 3], [2, 2856, 3], [2, 3], [2, 517, 6896, 3], [2, 2718, 3], [2, 2643, 3], [2, 3093, 3], [2, 1409, 3], [2, 3], [2, 517, 6827, 3], [2, 1562, 3], [2, 3], [2, 517, 6197, 3], [2, 3647, 3], [2, 3332, 3], [2, 1115, 3], [2, 3], [2, 517, 6116, 3], [2, 4743, 3], [2, 4998, 3], [2, 1185, 3], [2, 2514, 3], [2, 2181, 3], [2, 3], [2, 517, 7088, 3], [2, 3], [2, 517, 6301, 3], [2, 3], [2, 517, 7996, 3], [2, 993, 3], [2, 3886, 3], [2, 5017, 3], [2, 1562, 3], [2, 993, 3], [2, 1958, 3], [2, 5017, 3], [2, 1562, 3], [2, 3], [2, 517, 54, 3], [2, 3, 3], [2, 2, 3], [2, 3332, 3], [2, 1115, 3], [2, 4797, 3], [2, 3], [2, 517, 7086, 3], [2, 3], [2, 517, 388, 3], [2, 3], [2, 517, 425, 3], [2, 3], [2, 517, 367, 3], [2, 2468, 3], [2, 2732, 3], [2, 937, 3], [2, 3], [2, 517, 5468, 3], [2, 4698, 3], [2, 4635, 3], [2, 4693, 3], [2, 3093, 3], [2, 3], [2, 517, 7095, 3], [2, 921, 3], [2, 3], [2, 517, 5733, 3], [2, 522, 3], [2, 3574, 3], [2, 4012, 3], [2, 4491, 3], [2, 3], [2, 517, 40, 3], [2, 3647, 3], [2, 3298, 3], [2, 3], [2, 517, 6037, 3], [2, 2801, 3], [2, 1633, 3], [2, 3], [2, 517, 6896, 3], [2, 889, 3], [2, 3], [2, 517, 7443, 3], [2, 3886, 3], [2, 3332, 3], [2, 2734, 3], [2, 3], [2, 517, 7601, 3], [2, 3], [2, 517, 6896, 3], [2, 3], [2, 517, 7095, 3], [2, 4998, 3], [2, 4360, 3], [2, 5112, 3], [2, 4924, 3], [2, 2029, 3], [2, 2718, 3], [2, 2860, 3], [2, 2658, 3], [2, 1770, 3], [2, 3], [2, 517, 388, 3], [2, 3], [2, 517, 425, 3], [2, 3], [2, 517, 367, 3], [2, 3], [2, 517, 6116, 3], [2, 993, 3], [2, 4617, 3], [2, 993, 3], [2, 3886, 3], [2, 3468, 3], [2, 2734, 3], [2, 2860, 3], [2, 2658, 3], [2, 3], [2, 517, 7074, 3], [2, 3], [2, 517, 6079, 3], [2, 2423, 3], [2, 4728, 3], [2, 5152, 3], [2, 2401, 3], [2, 4924, 3], [2, 3], [2, 517, 5760, 3], [2, 1706, 3], [2, 3], [2, 517, 7589, 3], [2, 3], [2, 517, 7028, 3], [2, 4955, 3], [2, 3574, 3], [2, 4012, 3], [2, 3886, 3], [2, 1801, 3], [2, 3], [2, 517, 7088, 3], [2, 3], [2, 517, 6158, 3], [2, 3647, 3], [2, 825, 3], [2, 921, 3], [2, 1770, 3], [2, 905, 3], [2, 3], [2, 517, 7074, 3], [2, 3], [2, 517, 6079, 3], [2, 1370, 3], [2, 4698, 3], [2, 3], [2, 517, 5671, 3], [2, 1562, 3], [2, 993, 3], [2, 3], [2, 517, 6301, 3], [2, 3], [2, 517, 7919, 3], [2, 1562, 3], [2, 3], [2, 517, 54, 3], [2, 3, 3], [2, 2, 3], [2, 1884, 3], [2, 2929, 3], [2, 4693, 3], [2, 1103, 3], [2, 2929, 3], [2, 3], [2, 517, 5760, 3], [2, 518, 3], [2, 4698, 3], [2, 4635, 3], [2, 4693, 3], [2, 3093, 3], [2, 3], [2, 517, 5330, 3], [2, 5049, 3], [2, 3969, 3], [2, 2554, 3], [2, 3], [2, 517, 6113, 3], [2, 921, 3], [2, 4360, 3], [2, 5112, 3], [2, 4924, 3], [2, 3], [2, 517, 5760, 3], [2, 3574, 3], [2, 4012, 3], [2, 3886, 3], [2, 3], [2, 517, 5330, 3], [2, 3], [2, 517, 5330, 3], [2, 3954, 3], [2, 3], [2, 517, 6158, 3], [2, 3], [2, 517, 7086, 3], [2, 921, 3], [2, 3], [2, 517, 5733, 3], [2, 3], [2, 517, 7088, 3], [2, 3], [2, 517, 5330, 3], [2, 4297, 3], [2, 993, 3], [2, 3854, 3], [2, 1562, 3], [2, 3], [2, 517, 5760, 3], [2, 2573, 3], [2, 3036, 3], [2, 3], [2, 517, 7088, 3], [2, 2235, 3], [2, 936, 3], [2, 5017, 3], [2, 1562, 3], [2, 518, 3], [2, 993, 3], [2, 1958, 3], [2, 5017, 3], [2, 1562, 3], [2, 3], [2, 517, 54, 3], [2, 3, 3], [2, 2, 3], [2, 5037, 3], [2, 3647, 3], [2, 3], [2, 517, 7553, 3], [2, 1103, 3], [2, 2872, 3], [2, 3], [2, 517, 5760, 3], [2, 518, 3], [2, 4698, 3], [2, 4635, 3], [2, 4693, 3], [2, 3093, 3], [2, 3], [2, 517, 5760, 3], [2, 1986, 3], [2, 3], [2, 517, 5712, 3], [2, 624, 3], [2, 835, 3], [2, 3], [2, 517, 7028, 3], [2, 777, 3], [2, 1535, 3], [2, 3], [2, 517, 6896, 3], [2, 3], [2, 517, 5849, 3], [2, 3298, 3], [2, 3854, 3], [2, 993, 3], [2, 605, 3], [2, 835, 3], [2, 3], [2, 517, 7028, 3], [2, 3], [2, 517, 7086, 3], [2, 993, 3], [2, 807, 3], [2, 3], [2, 517, 5859, 3], [2, 3886, 3], [2, 3468, 3], [2, 2734, 3], [2, 3], [2, 517, 7088, 3], [2, 2220, 3], [2, 3], [2, 517, 5760, 3], [2, 1562, 3], [2, 518, 3], [2, 3], [2, 517, 6197, 3], [2, 518, 3], [2, 4698, 3], [2, 4635, 3], [2, 4693, 3], [2, 3093, 3], [2, 3], [2, 517, 5760, 3], [2, 3647, 3], [2, 3], [2, 517, 6896, 3], [2, 1633, 3], [2, 3612, 3], [2, 4998, 3], [2, 3574, 3], [2, 3332, 3], [2, 4955, 3], [2, 3], [2, 517, 5330, 3], [2, 4297, 3], [2, 3], [2, 517, 6474, 3], [2, 1258, 3], [2, 3], [2, 517, 6079, 3], [2, 1562, 3], [2, 3214, 3], [2, 4955, 3], [2, 3552, 3], [2, 4617, 3], [2, 3], [2, 517, 6896, 3], [2, 2718, 3], [2, 2643, 3], [2, 2872, 3], [2, 3854, 3], [2, 921, 3], [2, 3996, 3], [2, 3612, 3], [2, 5017, 3], [2, 993, 3], [2, 3647, 3], [2, 3], [2, 517, 6116, 3], [2, 4743, 3], [2, 4998, 3], [2, 4871, 3], [2, 2769, 3], [2, 3], [2, 517, 5468, 3], [2, 3886, 3], [2, 3468, 3], [2, 2734, 3], [2, 3], [2, 517, 7088, 3], [2, 936, 3], [2, 3], [2, 517, 5954, 3], [2, 2872, 3], [2, 3854, 3], [2, 3], [2, 517, 5760, 3], [2, 936, 3], [2, 993, 3], [2, 4955, 3], [2, 835, 3], [2, 4491, 3], [2, 1165, 3], [2, 1115, 3], [2, 4162, 3], [2, 3], [2, 517, 6116, 3], [2, 1931, 3], [2, 1801, 3], [2, 3], [2, 517, 6885, 3], [2, 1562, 3], [2, 518, 3], [2, 993, 3], [2, 2769, 3], [2, 2034, 3], [2, 5017, 3], [2, 1562, 3], [2, 3], [2, 517, 54, 3], [2, 3, 3], [2, 2, 3], [2, 3332, 3], [2, 1115, 3], [2, 4797, 3], [2, 3], [2, 517, 7086, 3], [2, 1861, 3], [2, 4698, 3], [2, 4635, 3], [2, 4693, 3], [2, 3093, 3], [2, 2468, 3], [2, 1897, 3], [2, 3], [2, 517, 6896, 3], [2, 1633, 3], [2, 4998, 3], [2, 2718, 3], [2, 3], [2, 517, 5859, 3], [2, 3647, 3], [2, 1369, 3], [2, 3758, 3], [2, 4297, 3], [2, 3], [2, 517, 46, 3], [2, 4162, 3], [2, 1897, 3], [2, 3758, 3], [2, 4297, 3], [2, 1815, 3], [2, 3], [2, 517, 6896, 3], [2, 1633, 3], [2, 4955, 3], [2, 1498, 3], [2, 3], [2, 517, 6016, 3], [2, 3647, 3], [2, 3854, 3], [2, 3], [2, 517, 6885, 3], [2, 3], [2, 517, 7074, 3], [2, 1370, 3], [2, 3647, 3], [2, 2307, 3], [2, 921, 3], [2, 3], [2, 517, 5733, 3], [2, 2468, 3], [2, 2732, 3], [2, 3], [2, 517, 7074, 3], [2, 3], [2, 517, 6079, 3], [2, 3647, 3], [2, 1369, 3], [2, 1875, 3], [2, 3], [2, 517, 5760, 3], [2, 921, 3], [2, 5124, 3], [2, 3758, 3], [2, 3], [2, 517, 5878, 3], [2, 1562, 3], [2, 3], [2, 517, 6197, 3], [2, 2959, 3], [2, 777, 3], [2, 5196, 3], [2, 3], [2, 517, 6117, 3], [2, 3], [2, 517, 6896, 3], [2, 1833, 3], [2, 1875, 3], [2, 921, 3], [2, 3], [2, 517, 5733, 3], [2, 3647, 3], [2, 4681, 3], [2, 921, 3], [2, 2339, 3], [2, 5017, 3], [2, 3606, 3], [2, 3], [2, 517, 6896, 3], [2, 3], [2, 517, 5859, 3], [2, 3006, 3], [2, 2135, 3], [2, 4491, 3], [2, 5081, 3], [2, 4720, 3], [2, 3], [2, 517, 5330, 3], [2, 862, 3], [2, 3], [2, 517, 7095, 3], [2, 2339, 3], [2, 4924, 3], [2, 4297, 3], [2, 3146, 3], [2, 3], [2, 517, 7086, 3], [2, 4074, 3], [2, 3], [2, 517, 7086, 3], [2, 2695, 3], [2, 3332, 3], [2, 1115, 3], [2, 3], [2, 517, 5468, 3], [2, 4128, 3], [2, 1875, 3], [2, 993, 3], [2, 3], [2, 517, 6301, 3], [2, 3], [2, 517, 7919, 3], [2, 1562, 3], [2, 3], [2, 517, 54, 3], [2, 3, 3], [2, 2, 3], [2, 3332, 3], [2, 1115, 3], [2, 4797, 3], [2, 3], [2, 517, 7086, 3], [2, 1185, 3], [2, 3], [2, 517, 6037, 3], [2, 1370, 3], [2, 4698, 3], [2, 4635, 3], [2, 4693, 3], [2, 3093, 3], [2, 3], [2, 517, 5330, 3], [2, 3], [2, 517, 5468, 3], [2, 862, 3], [2, 5136, 3], [2, 953, 3], [2, 2339, 3], [2, 5112, 3], [2, 3], [2, 517, 6896, 3], [2, 2554, 3], [2, 3], [2, 517, 6113, 3], [2, 921, 3], [2, 3996, 3], [2, 3612, 3], [2, 4998, 3], [2, 2643, 3], [2, 3093, 3], [2, 1409, 3], [2, 3], [2, 517, 6827, 3], [2, 4297, 3], [2, 1931, 3], [2, 5049, 3], [2, 3969, 3], [2, 3], [2, 517, 7095, 3], [2, 3437, 3], [2, 1406, 3], [2, 5112, 3], [2, 3], [2, 517, 6983, 3], [2, 2718, 3], [2, 3006, 3], [2, 4297, 3], [2, 784, 3], [2, 2822, 3], [2, 2856, 3], [2, 3], [2, 517, 5859, 3], [2, 3], [2, 517, 6116, 3], [2, 993, 3], [2, 3], [2, 517, 6060, 3], [2, 4924, 3], [2, 2029, 3], [2, 3184, 3], [2, 3], [2, 517, 7074, 3], [2, 3], [2, 517, 6079, 3], [2, 529, 3], [2, 526, 3], [2, 526, 3], [2, 3], [2, 517, 5712, 3], [2, 3647, 3], [2, 2658, 3], [2, 2643, 3], [2, 3093, 3], [2, 1409, 3], [2, 1258, 3], [2, 3], [2, 517, 5760, 3], [2, 3220, 3], [2, 3], [2, 517, 6060, 3], [2, 3524, 3], [2, 905, 3], [2, 3], [2, 517, 7074, 3], [2, 3], [2, 517, 6079, 3], [2, 2355, 3], [2, 3758, 3], [2, 1562, 3], [2, 993, 3], [2, 3498, 3], [2, 3], [2, 517, 6060, 3], [2, 5017, 3], [2, 1562, 3], [2, 3], [2, 517, 54, 3], [2, 3, 3], [2, 2, 3], [2, 3332, 3], [2, 1115, 3], [2, 3], [2, 517, 5330, 3], [2, 4360, 3], [2, 5022, 3], [2, 1763, 3], [2, 3], [2, 517, 5760, 3], [2, 1741, 3], [2, 3135, 3], [2, 3], [2, 517, 7673, 3], [2, 2333, 3], [2, 4773, 3], [2, 993, 3], [2, 3533, 3], [2, 3], [2, 517, 7095, 3], [2, 4698, 3], [2, 4635, 3], [2, 4693, 3], [2, 3093, 3], [2, 835, 3], [2, 4491, 3], [2, 2872, 3], [2, 3], [2, 517, 5760, 3], [2, 1986, 3], [2, 3], [2, 517, 5712, 3], [2, 529, 3], [2, 3], [2, 517, 54, 3], [2, 617, 3], [2, 3], [2, 517, 11, 3], [2, 3], [2, 517, 6792, 3], [2, 784, 3], [2, 2822, 3], [2, 5017, 3], [2, 3], [2, 517, 7074, 3], [2, 3], [2, 517, 6197, 3], [2, 2718, 3], [2, 3006, 3], [2, 4297, 3], [2, 3], [2, 517, 5859, 3], [2, 2554, 3], [2, 3], [2, 517, 6113, 3], [2, 921, 3], [2, 4252, 3], [2, 3220, 3], [2, 1235, 3], [2, 2801, 3], [2, 1258, 3], [2, 1958, 3], [2, 3], [2, 517, 6896, 3], [2, 3], [2, 517, 5760, 3], [2, 4698, 3], [2, 4635, 3], [2, 4693, 3], [2, 3093, 3], [2, 3], [2, 517, 6896, 3], [2, 3996, 3], [2, 4984, 3], [2, 4955, 3], [2, 2718, 3], [2, 3006, 3], [2, 4297, 3], [2, 3], [2, 517, 5330, 3], [2, 2801, 3], [2, 980, 3], [2, 3996, 3], [2, 3], [2, 517, 7074, 3], [2, 3], [2, 517, 6079, 3], [2, 529, 3], [2, 4471, 3], [2, 3], [2, 517, 463, 3], [2, 529, 3], [2, 4471, 3], [2, 611, 3], [2, 526, 3], [2, 526, 3], [2, 3], [2, 517, 418, 3], [2, 553, 3], [2, 3], [2, 517, 6284, 3], [2, 3], [2, 517, 6896, 3], [2, 1409, 3], [2, 4297, 3], [2, 3146, 3], [2, 3], [2, 517, 7088, 3], [2, 905, 3], [2, 3], [2, 517, 7074, 3], [2, 3], [2, 517, 6079, 3], [2, 3396, 3], [2, 2658, 3], [2, 3], [2, 517, 5878, 3], [2, 1562, 3], [2, 3], [2, 517, 54, 3], [2, 3, 3], [2, 2, 3], [2, 3332, 3], [2, 1115, 3], [2, 4797, 3], [2, 3], [2, 517, 7086, 3], [2, 4698, 3], [2, 4635, 3], [2, 4693, 3], [2, 3093, 3], [2, 3], [2, 517, 7095, 3], [2, 3], [2, 517, 6202, 3], [2, 4197, 3], [2, 3], [2, 517, 7088, 3], [2, 1927, 3], [2, 1258, 3], [2, 3552, 3], [2, 4998, 3], [2, 3036, 3], [2, 3], [2, 517, 7892, 3], [2, 3036, 3], [2, 3], [2, 517, 6896, 3], [2, 2718, 3], [2, 4698, 3], [2, 4635, 3], [2, 4693, 3], [2, 3093, 3], [2, 3], [2, 517, 6116, 3], [2, 4285, 3], [2, 3006, 3], [2, 4955, 3], [2, 1562, 3], [2, 3606, 3], [2, 3], [2, 517, 7673, 3], [2, 2333, 3], [2, 4773, 3], [2, 993, 3], [2, 3533, 3], [2, 3], [2, 517, 6896, 3], [2, 3647, 3], [2, 3006, 3], [2, 4924, 3], [2, 3], [2, 517, 5760, 3], [2, 2959, 3], [2, 3], [2, 517, 5859, 3], [2, 3], [2, 517, 6116, 3], [2, 4924, 3], [2, 993, 3], [2, 3854, 3], [2, 3], [2, 517, 7074, 3], [2, 3], [2, 517, 6197, 3], [2, 611, 3], [2, 3], [2, 517, 5712, 3], [2, 777, 3], [2, 1073, 3], [2, 3], [2, 517, 7397, 3], [2, 937, 3], [2, 3], [2, 517, 5468, 3], [2, 3647, 3], [2, 3006, 3], [2, 1770, 3], [2, 3006, 3], [2, 2135, 3], [2, 3803, 3], [2, 2423, 3], [2, 3], [2, 517, 5330, 3], [2, 2704, 3], [2, 4192, 3], [2, 4924, 3], [2, 993, 3], [2, 2307, 3], [2, 2781, 3], [2, 4924, 3], [2, 3], [2, 517, 5760, 3], [2, 905, 3], [2, 3], [2, 517, 7074, 3], [2, 3], [2, 517, 6079, 3], [2, 1370, 3], [2, 4698, 3], [2, 3], [2, 517, 5671, 3], [2, 1562, 3], [2, 993, 3], [2, 3], [2, 517, 6301, 3], [2, 3], [2, 517, 7919, 3], [2, 1562, 3], [2, 3], [2, 517, 54, 3], [2, 3, 3], [2, 2, 3], [2, 1884, 3], [2, 2929, 3], [2, 4693, 3], [2, 1103, 3], [2, 2872, 3], [2, 3], [2, 517, 5760, 3], [2, 518, 3], [2, 3758, 3], [2, 777, 3], [2, 3647, 3], [2, 4360, 3], [2, 5112, 3], [2, 4092, 3], [2, 4074, 3], [2, 3], [2, 517, 6896, 3], [2, 3854, 3], [2, 1562, 3], [2, 993, 3], [2, 2704, 3], [2, 773, 3], [2, 4924, 3], [2, 4297, 3], [2, 1931, 3], [2, 1023, 3], [2, 3], [2, 517, 6094, 3], [2, 3], [2, 517, 5859, 3], [2, 3], [2, 517, 6989, 3], [2, 1562, 3], [2, 3], [2, 517, 5330, 3], [2, 2573, 3], [2, 1875, 3], [2, 4360, 3], [2, 905, 3], [2, 4464, 3], [2, 3], [2, 517, 6043, 3], [2, 3758, 3], [2, 777, 3], [2, 3], [2, 517, 5859, 3], [2, 2573, 3], [2, 1875, 3], [2, 4379, 3], [2, 2872, 3], [2, 3854, 3], [2, 1562, 3], [2, 518, 3], [2, 3], [2, 517, 6197, 3], [2, 518, 3], [2, 1023, 3], [2, 3], [2, 517, 6094, 3], [2, 3], [2, 517, 7095, 3], [2, 1815, 3], [2, 3954, 3], [2, 3], [2, 517, 5468, 3], [2, 3], [2, 517, 6202, 3], [2, 4197, 3], [2, 3], [2, 517, 46, 3], [2, 3758, 3], [2, 777, 3], [2, 3], [2, 517, 7095, 3], [2, 1815, 3], [2, 3954, 3], [2, 3], [2, 517, 7088, 3], [2, 4297, 3], [2, 3], [2, 517, 7530, 3], [2, 2408, 3], [2, 4698, 3], [2, 4635, 3], [2, 4693, 3], [2, 3093, 3], [2, 3], [2, 517, 6079, 3], [2, 2423, 3], [2, 4728, 3], [2, 5152, 3], [2, 2401, 3], [2, 3], [2, 517, 6064, 3], [2, 3], [2, 517, 5468, 3], [2, 3], [2, 517, 6202, 3], [2, 4197, 3], [2, 3], [2, 517, 6896, 3], [2, 1633, 3], [2, 4998, 3], [2, 2095, 3], [2, 3241, 3], [2, 3], [2, 517, 5330, 3], [2, 3], [2, 517, 6116, 3], [2, 2287, 3], [2, 3524, 3], [2, 2872, 3], [2, 3854, 3], [2, 3], [2, 517, 7088, 3], [2, 905, 3], [2, 518, 3], [2, 3647, 3], [2, 1875, 3], [2, 993, 3], [2, 1958, 3], [2, 5017, 3], [2, 1562, 3], [2, 3], [2, 517, 54, 3]]\n"
     ]
    }
   ],
   "source": [
    "vocab = tokenizer.get_vocab()\n",
    "print(vocab.get('▁('))\n",
    "print([tokenizer.encode(token) for token in src_subtokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'▁('"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_subtokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_subtokens = src_subtokens[:510]  ## 512가 최대인데 [SEP], [CLS] 2개 때문에 510\n",
    "len(src_subtokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_subtokens = ['[CLS]'] + src_subtokens + ['[SEP]']\n",
    "len(src_subtokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 517,\n",
       " 380,\n",
       " 517,\n",
       " 425,\n",
       " 517,\n",
       " 415,\n",
       " 953,\n",
       " 1409,\n",
       " 517,\n",
       " 7086,\n",
       " 5022,\n",
       " 777,\n",
       " 2423,\n",
       " 517,\n",
       " 7095,\n",
       " 611,\n",
       " 617,\n",
       " 553,\n",
       " 517,\n",
       " 6858,\n",
       " 3533,\n",
       " 517,\n",
       " 7964,\n",
       " 517,\n",
       " 6077,\n",
       " 2573,\n",
       " 881,\n",
       " 3647,\n",
       " 4451,\n",
       " 517,\n",
       " 7145,\n",
       " 517,\n",
       " 7996,\n",
       " 1258,\n",
       " 517,\n",
       " 5859,\n",
       " 4012,\n",
       " 3758,\n",
       " 1706,\n",
       " 1861,\n",
       " 1562,\n",
       " 517,\n",
       " 6115,\n",
       " 1235,\n",
       " 517,\n",
       " 7073,\n",
       " 2573,\n",
       " 993,\n",
       " 517,\n",
       " 5330,\n",
       " 3332,\n",
       " 2235,\n",
       " 4924,\n",
       " 993,\n",
       " 3854,\n",
       " 1562,\n",
       " 517,\n",
       " 54,\n",
       " 3,\n",
       " 2,\n",
       " 2872,\n",
       " 2322,\n",
       " 517,\n",
       " 5859,\n",
       " 862,\n",
       " 517,\n",
       " 6023,\n",
       " 1258,\n",
       " 3260,\n",
       " 517,\n",
       " 7095,\n",
       " 2149,\n",
       " 1023,\n",
       " 835,\n",
       " 4092,\n",
       " 2355,\n",
       " 517,\n",
       " 6079,\n",
       " 4213,\n",
       " 3006,\n",
       " 517,\n",
       " 7088,\n",
       " 862,\n",
       " 517,\n",
       " 6023,\n",
       " 4998,\n",
       " 2423,\n",
       " 1618,\n",
       " 3647,\n",
       " 517,\n",
       " 5926,\n",
       " 517,\n",
       " 7088,\n",
       " 4598,\n",
       " 4924,\n",
       " 993,\n",
       " 517,\n",
       " 46,\n",
       " 993,\n",
       " 517,\n",
       " 5370,\n",
       " 2081,\n",
       " 517,\n",
       " 6023,\n",
       " 980,\n",
       " 4211,\n",
       " 517,\n",
       " 6116,\n",
       " 835,\n",
       " 2769,\n",
       " 4924,\n",
       " 517,\n",
       " 5760,\n",
       " 1815,\n",
       " 1535,\n",
       " 3184,\n",
       " 517,\n",
       " 7095,\n",
       " 3647,\n",
       " 3757,\n",
       " 517,\n",
       " 7088,\n",
       " 3552,\n",
       " 4998,\n",
       " 3437,\n",
       " 825,\n",
       " 2485,\n",
       " 2322,\n",
       " 517,\n",
       " 478,\n",
       " 4832,\n",
       " 2322,\n",
       " 5022,\n",
       " 3552,\n",
       " 517,\n",
       " 5330,\n",
       " 1741,\n",
       " 3533,\n",
       " 1763,\n",
       " 993,\n",
       " 3854,\n",
       " 1562,\n",
       " 517,\n",
       " 54,\n",
       " 3,\n",
       " 2,\n",
       " 517,\n",
       " 415,\n",
       " 517,\n",
       " 380,\n",
       " 1132,\n",
       " 2169,\n",
       " 517,\n",
       " 7086,\n",
       " 5022,\n",
       " 517,\n",
       " 7095,\n",
       " 4349,\n",
       " 3533,\n",
       " 1801,\n",
       " 3647,\n",
       " 993,\n",
       " 517,\n",
       " 5370,\n",
       " 2573,\n",
       " 517,\n",
       " 7095,\n",
       " 2149,\n",
       " 1023,\n",
       " 835,\n",
       " 4092,\n",
       " 2355,\n",
       " 517,\n",
       " 6116,\n",
       " 3647,\n",
       " 3494,\n",
       " 4998,\n",
       " 4213,\n",
       " 3006,\n",
       " 517,\n",
       " 7088,\n",
       " 2573,\n",
       " 993,\n",
       " 4814,\n",
       " 3093,\n",
       " 529,\n",
       " 553,\n",
       " 621,\n",
       " 517,\n",
       " 6858,\n",
       " 3533,\n",
       " 517,\n",
       " 7095,\n",
       " 2423,\n",
       " 1618,\n",
       " 3647,\n",
       " 517,\n",
       " 5926,\n",
       " 517,\n",
       " 7088,\n",
       " 517,\n",
       " 7415,\n",
       " 1311,\n",
       " 2573,\n",
       " 3036,\n",
       " 3647,\n",
       " 4519,\n",
       " 1221,\n",
       " 1235,\n",
       " 517,\n",
       " 7073,\n",
       " 1618,\n",
       " 1132,\n",
       " 517,\n",
       " 6896,\n",
       " 3996,\n",
       " 2235,\n",
       " 517,\n",
       " 5878,\n",
       " 1562,\n",
       " 517,\n",
       " 54,\n",
       " 3,\n",
       " 2,\n",
       " 3647,\n",
       " 1801,\n",
       " 517,\n",
       " 7086,\n",
       " 553,\n",
       " 526,\n",
       " 553,\n",
       " 529,\n",
       " 517,\n",
       " 5712,\n",
       " 529,\n",
       " 517,\n",
       " 7028,\n",
       " 2423,\n",
       " 4728,\n",
       " 3439,\n",
       " 4998,\n",
       " 605,\n",
       " 517,\n",
       " 7028,\n",
       " 517,\n",
       " 5591,\n",
       " 4297,\n",
       " 617,\n",
       " 529,\n",
       " 835,\n",
       " 2658,\n",
       " 3954,\n",
       " 2573,\n",
       " 517,\n",
       " 7095,\n",
       " 4285,\n",
       " 1170,\n",
       " 3260,\n",
       " 2095,\n",
       " 517,\n",
       " 6116,\n",
       " 1633,\n",
       " 5022,\n",
       " 4924,\n",
       " 517,\n",
       " 6197,\n",
       " 3166,\n",
       " 921,\n",
       " 1770,\n",
       " 2095,\n",
       " 2658,\n",
       " 4285,\n",
       " 3886,\n",
       " 517,\n",
       " 5532,\n",
       " 2044,\n",
       " 517,\n",
       " 6983,\n",
       " 3803,\n",
       " 4092,\n",
       " 517,\n",
       " 7088,\n",
       " 4213,\n",
       " 3006,\n",
       " 1986,\n",
       " 2872,\n",
       " 517,\n",
       " 6896,\n",
       " 3647,\n",
       " 3494,\n",
       " 5017,\n",
       " 1562,\n",
       " 517,\n",
       " 54,\n",
       " 3,\n",
       " 2,\n",
       " 4092,\n",
       " 2355,\n",
       " 1023,\n",
       " 835,\n",
       " 4012,\n",
       " 2149,\n",
       " 1900,\n",
       " 4213,\n",
       " 3006,\n",
       " 517,\n",
       " 7088,\n",
       " 2573,\n",
       " 517,\n",
       " 5915,\n",
       " 1562,\n",
       " 517,\n",
       " 5330,\n",
       " 1023,\n",
       " 2959,\n",
       " 1783,\n",
       " 4213,\n",
       " 517,\n",
       " 5330,\n",
       " 517,\n",
       " 5330,\n",
       " 3417,\n",
       " 517,\n",
       " 6113,\n",
       " 2029,\n",
       " 4814,\n",
       " 517,\n",
       " 6827,\n",
       " 1562,\n",
       " 517,\n",
       " 54,\n",
       " 3,\n",
       " 2,\n",
       " 2095,\n",
       " 2658,\n",
       " 4285,\n",
       " 3886,\n",
       " 517,\n",
       " 6116,\n",
       " 4924,\n",
       " 921,\n",
       " 1763,\n",
       " 2029,\n",
       " 1258,\n",
       " 3260,\n",
       " 3969,\n",
       " 2095,\n",
       " 1115,\n",
       " 4162,\n",
       " 517,\n",
       " 5330,\n",
       " 881,\n",
       " 4012,\n",
       " 4955,\n",
       " 905,\n",
       " 517,\n",
       " 7074,\n",
       " 517,\n",
       " 6079,\n",
       " 4888,\n",
       " 3647,\n",
       " 1761,\n",
       " 4213,\n",
       " 517,\n",
       " 5330,\n",
       " 517,\n",
       " 6896,\n",
       " 517,\n",
       " 5760,\n",
       " 5090,\n",
       " 3969,\n",
       " 517,\n",
       " 6079,\n",
       " 3930,\n",
       " 3494,\n",
       " 4955,\n",
       " 1562,\n",
       " 517,\n",
       " 54,\n",
       " 3,\n",
       " 2,\n",
       " 3647,\n",
       " 517,\n",
       " 6040,\n",
       " 2267,\n",
       " 3006,\n",
       " 517,\n",
       " 7074,\n",
       " 517,\n",
       " 6079,\n",
       " 617,\n",
       " 617,\n",
       " 517,\n",
       " 6858,\n",
       " 3533,\n",
       " 4092,\n",
       " 517,\n",
       " 5859,\n",
       " 517,\n",
       " 6116,\n",
       " 517,\n",
       " 7415,\n",
       " 517,\n",
       " 5422,\n",
       " 993,\n",
       " 517,\n",
       " 46,\n",
       " 3803,\n",
       " 2423,\n",
       " 517,\n",
       " 5760,\n",
       " 1562,\n",
       " 517,\n",
       " 6115,\n",
       " 2423,\n",
       " 2718,\n",
       " 517,\n",
       " 7095,\n",
       " 1741,\n",
       " 517,\n",
       " 6093,\n",
       " 1370,\n",
       " 517,\n",
       " 5330,\n",
       " 517,\n",
       " 7263,\n",
       " 1815,\n",
       " 517,\n",
       " 6896,\n",
       " 921,\n",
       " 517,\n",
       " 5859,\n",
       " 4092,\n",
       " 2355,\n",
       " 517,\n",
       " 6116,\n",
       " 4012,\n",
       " 1597,\n",
       " 5017,\n",
       " 1562,\n",
       " 517,\n",
       " 54,\n",
       " 3,\n",
       " 2,\n",
       " 3647,\n",
       " 517,\n",
       " 5468,\n",
       " 4092,\n",
       " 517,\n",
       " 6896,\n",
       " 2718,\n",
       " 517,\n",
       " 5859,\n",
       " 617,\n",
       " 529,\n",
       " 517,\n",
       " 6858,\n",
       " 3533,\n",
       " 2658,\n",
       " 1618,\n",
       " 517,\n",
       " 7095,\n",
       " 2423,\n",
       " 1618,\n",
       " 3647,\n",
       " 517,\n",
       " 5926,\n",
       " 3647,\n",
       " 2235,\n",
       " 2704,\n",
       " 5017,\n",
       " 1562,\n",
       " 517,\n",
       " 54,\n",
       " 3,\n",
       " 2,\n",
       " 1633,\n",
       " 1115,\n",
       " 517,\n",
       " 7086,\n",
       " 5022,\n",
       " 3803,\n",
       " 2423,\n",
       " 4297,\n",
       " 4074,\n",
       " 4349,\n",
       " 3533,\n",
       " 2872,\n",
       " 517,\n",
       " 6749,\n",
       " 2034,\n",
       " 517,\n",
       " 7086,\n",
       " 4841,\n",
       " 517,\n",
       " 5330,\n",
       " 3036,\n",
       " 3996,\n",
       " 517,\n",
       " 7088,\n",
       " 3439,\n",
       " 1900,\n",
       " 1258,\n",
       " 3552,\n",
       " 4998,\n",
       " 4297,\n",
       " 1406,\n",
       " 4998,\n",
       " 529,\n",
       " 526,\n",
       " 3]"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_subtoken_idxs = tokenizer.convert_tokens_to_ids(src_subtokens)\n",
    "src_subtoken_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text) # 원문 문장 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_subtoken_idxs.count(2)  # 2는 [CLS] 토큰\n",
    "# 즉 9개의 문장에 해당하는 [CLS] 토큰만 입력된다는 이야기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_segs = [-1] + [i for i, t in enumerate(src_subtoken_idxs) if t == self.sep_vid]\n",
    "segs = [_segs[i] - _segs[i - 1] for i in range(1, len(_segs))]\n",
    "segments_ids = []\n",
    "for i, s in enumerate(segs):\n",
    "    if (i % 2 == 0):\n",
    "        segments_ids += s * [0]\n",
    "    else:\n",
    "        segments_ids += s * [1]\n",
    "cls_ids = [i for i, t in enumerate(src_subtoken_idxs) if t == self.cls_vid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "일본 정부가 문재인 대통령이 도쿄올림픽을 계기로 일본을 방문하는 방향으로 한일 양국이 조율하고 있다는 15일 요미우리신문의 보도를 부인했다.\n",
    "\n",
    "정부 대변인인 가토 가쓰노부(加藤勝信) 관방장관은 이날 오전 정례 기자회견에서 관련 질문에 “말씀하신 보도와 같은 사실이 없는 것으로 안다”고 밝혔다.\n",
    "\n",
    "앞서 요미우리는 한국 측이 도쿄올림픽을 계기로 한 문 대통령의 방일을 타진했고, 일본 측은 수용하는 방향이라고 이날 보도했다.\n",
    "\n",
    "한국 측은 문 대통령의 방일 때 스가 요시히데(菅義偉) 총리와 처음으로 정상회담을 하겠다는 생각이라고 요미우리는 전했다.\n",
    "\n",
    "가토 장관은 한일 정상회담에 대한 일본 정부의 자세에 대해 “그런 사실이 없기 때문에 가정의 질문에 대해 답하는 것을 삼가겠다”고 말했다.\n",
    "\n",
    "그는 한국 측의 독도방어훈련에 ‘어떤 대항 조치를 생각하고 있느냐’는 질문에는 “한국 해군의 훈련에 대해 정부로서는 강한 관심을 가지고 주시하는 상황이어서 지금 시점에선 논평을 삼가겠다”고 말을 아꼈다.\n",
    "\n",
    "가토 장관은 “다케시마(竹島·일본이 주장하는 독도의 명칭)는 역사적 사실에 비춰봐도, 국제법상으로도 명백한 일본 고유의 영토”라며 독도 영유권 주장을 되풀이했다.\n",
    "\n",
    "그러면서 “다케시마 문제에 대해서는 계속 우리나라의 영토, 영해, 영공을 단호히 지키겠다는 결의로 냉정하고 의연하게 대응해갈 생각”이라고 밝혔다.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['일본 정부가 문재인 대통령이 도쿄올림픽을 계기로 일본을 방문하는 방향으로 한일 양국이 조율하고 있다는 15일 요미우리신문의 보도를 부인했다.',\n",
       " '정부 대변인인 가토 가쓰노부(加藤勝信) 관방장관은 이날 오전 정례 기자회견에서 관련 질문에 “말씀하신 보도와 같은 사실이 없는 것으로 안다”고 밝혔다.',\n",
       " '앞서 요미우리는 한국 측이 도쿄올림픽을 계기로 한 문 대통령의 방일을 타진했고, 일본 측은 수용하는 방향이라고 이날 보도했다.',\n",
       " '한국 측은 문 대통령의 방일 때 스가 요시히데(菅義偉) 총리와 처음으로 정상회담을 하겠다는 생각이라고 요미우리는 전했다.',\n",
       " '가토 장관은 한일 정상회담에 대한 일본 정부의 자세에 대해 “그런 사실이 없기 때문에 가정의 질문에 대해 답하는 것을 삼가겠다”고 말했다.',\n",
       " '그는 한국 측의 독도방어훈련에 ‘어떤 대항 조치를 생각하고 있느냐’는 질문에는 “한국 해군의 훈련에 대해 정부로서는 강한 관심을 가지고 주시하는 상황이어서 지금 시점에선 논평을 삼가겠다”고 말을 아꼈다.',\n",
       " '가토 장관은 “다케시마(竹島·일본이 주장하는 독도의 명칭)는 역사적 사실에 비춰봐도, 국제법상으로도 명백한 일본 고유의 영토”라며 독도 영유권 주장을 되풀이했다.',\n",
       " '그러면서 “다케시마 문제에 대해서는 계속 우리나라의 영토, 영해, 영공을 단호히 지키겠다는 결의로 냉정하고 의연하게 대응해갈 생각”이라고 밝혔다.']"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = list(filter(None, text.split('\\n')))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertData():\n",
    "    def __init__(self):\n",
    "        self.tokenizer = KoBertTokenizer.from_pretrained(\"monologg/kobert\", do_lower_case=True)\n",
    "\n",
    "        self.sep_token = '[SEP]'\n",
    "        self.cls_token = '[CLS]'\n",
    "        self.pad_token = '[PAD]'\n",
    "        self.sep_vid = self.tokenizer.token2idx[self.sep_token]\n",
    "        self.cls_vid = self.tokenizer.token2idx[self.cls_token]\n",
    "        self.pad_vid = self.tokenizer.token2idx[self.pad_token]\n",
    "\n",
    "    def preprocess(self, src):\n",
    "\n",
    "        if (len(src) == 0):\n",
    "            return None\n",
    "\n",
    "        original_src_txt = [' '.join(s) for s in src]\n",
    "\n",
    "        labels = [0] * len(src)\n",
    "        # for l in oracle_ids:\n",
    "        #     labels[l] = 1\n",
    "\n",
    "        idxs = [i for i, s in enumerate(src) if (len(s) > 1)]\n",
    "\n",
    "        src = [src[i][:1000] for i in idxs]\n",
    "        #labels = [labels[i] for i in idxs]\n",
    "        src = src[:2000]\n",
    "        #labels = labels[:self.args.max_nsents]\n",
    "\n",
    "        # if (len(src) < 1):\n",
    "        #     return None\n",
    "        # if (len(labels) == 0):\n",
    "        #     return None\n",
    "\n",
    "        src_txt = [' '.join(sent) for sent in src]\n",
    "        # text = [' '.join(ex['src_txt'][i].split()[:self.args.max_src_ntokens]) for i in idxs]\n",
    "        # text = [_clean(t) for t in text]\n",
    "        text = ' [SEP] [CLS] '.join(src_txt)\n",
    "        src_subtokens = self.tokenizer.tokenize(text)\n",
    "        src_subtokens = src_subtokens[:510]\n",
    "        src_subtokens = ['[CLS]'] + src_subtokens + ['[SEP]']\n",
    "\n",
    "        src_subtoken_idxs = self.tokenizer.convert_tokens_to_ids(src_subtokens)\n",
    "        _segs = [-1] + [i for i, t in enumerate(src_subtoken_idxs) if t == self.sep_vid]\n",
    "        segs = [_segs[i] - _segs[i - 1] for i in range(1, len(_segs))]\n",
    "        segments_ids = []\n",
    "        for i, s in enumerate(segs):\n",
    "            if (i % 2 == 0):\n",
    "                segments_ids += s * [0]\n",
    "            else:\n",
    "                segments_ids += s * [1]\n",
    "        cls_ids = [i for i, t in enumerate(src_subtoken_idxs) if t == self.cls_vid]\n",
    "        #labels = labels[:len(cls_ids)]\n",
    "\n",
    "        #tgt_txt = '<q>'.join([' '.join(tt) for tt in tgt])\n",
    "        src_txt = [original_src_txt[i] for i in idxs]\n",
    "        return src_subtoken_idxs, segments_ids, cls_ids, src_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt2input(text):\n",
    "    data = list(filter(None, text.split('\\n')))\n",
    "    #data = split_sentences(text)\n",
    "    bertdata = BertData()\n",
    "    txt_data = bertdata.preprocess(data)\n",
    "    data_dict = {\"src\":txt_data[0],\n",
    "                \"segs\":txt_data[1],\n",
    "                \"clss\":txt_data[2],\n",
    "                \"src_txt\":txt_data[3]}\n",
    "    input_data = []\n",
    "    input_data.append(data_dict)\n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'KoBertTokenizer'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = txt2input(text)\n",
    "input_data[0]['src'].count(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'KoBertTokenizer'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(512, 6, 512)"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = txt2input(text)\n",
    "len(inputs[0]['src']), len(inputs[0]['clss']), len(inputs[0]['segs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "def _pad(data, pad_id, width=-1):\n",
    "    if (width == -1):\n",
    "        width = max(len(d) for d in data)\n",
    "    rtn_data = [d + [pad_id] * (width - len(d)) for d in data]\n",
    "    return rtn_data\n",
    "\n",
    "pre_src = [x['src'] for x in inputs]\n",
    "pre_segs = [x['segs'] for x in inputs]\n",
    "pre_clss = [x['clss'] for x in inputs]\n",
    "\n",
    "src = torch.tensor(_pad(pre_src, 0))\n",
    "segs = torch.tensor(_pad(pre_segs, 0))\n",
    "mask_src = ~(src == 0)\n",
    "\n",
    "clss = torch.tensor(_pad(pre_clss, -1))\n",
    "mask_cls = ~(clss == -1)\n",
    "clss[clss == -1] = 0\n",
    "\n",
    "clss.to(device).long()\n",
    "mask_cls.to(device).long()\n",
    "segs.to(device).long()\n",
    "mask_src.to(device).long()\n",
    "\n",
    "# checkpoint = torch.load(\"D:/KoBertSum/ext/models/model_step_26000.pt\")\n",
    "# model = ExtSummarizer(args, device, checkpoint)\n",
    "# model.eval()\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     sent_scores, mask = model(src, segs, clss, mask_src, mask_cls)\n",
    "#     print(sent_scores)\n",
    "#     sent_scores = sent_scores + mask.float()\n",
    "#     sent_scores = sent_scores.cpu().data.numpy()\n",
    "#     print(sent_scores)\n",
    "#     selected_ids = np.argsort(-sent_scores, 1)\n",
    "#     print(selected_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2849,  0.0282,  0.2609,  ..., -0.0735, -0.0258,  0.3375],\n",
       "         [ 0.0772, -0.1348, -0.0158,  ...,  0.0387, -0.1104,  0.0541],\n",
       "         [ 0.0997, -0.3027,  0.2173,  ..., -0.0441, -0.0477,  0.2517],\n",
       "         ...,\n",
       "         [ 0.0992, -0.3319,  0.0847,  ...,  0.0129,  0.1108,  0.2435],\n",
       "         [ 0.0923, -0.2898,  0.2338,  ..., -0.3074,  0.1949, -0.0939],\n",
       "         [-0.0438, -0.2441, -0.1525,  ..., -0.3074, -0.3792,  0.0161]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertModel.from_pretrained(\"monologg/kobert\")\n",
    "top_vec = model(input_ids=src, token_type_ids=segs, attention_mask=mask_src)[0]\n",
    "top_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 512]), torch.Size([1, 6]), torch.Size([1, 512]))"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src.size(), clss.size(), segs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0, 110, 290, 374, 432, 486]])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.2849,  0.0282,  0.2609,  ..., -0.0735, -0.0258,  0.3375],\n",
       "          [ 0.0772, -0.1348, -0.0158,  ...,  0.0387, -0.1104,  0.0541],\n",
       "          [ 0.0997, -0.3027,  0.2173,  ..., -0.0441, -0.0477,  0.2517],\n",
       "          ...,\n",
       "          [ 0.0992, -0.3319,  0.0847,  ...,  0.0129,  0.1108,  0.2435],\n",
       "          [ 0.0923, -0.2898,  0.2338,  ..., -0.3074,  0.1949, -0.0939],\n",
       "          [-0.0438, -0.2441, -0.1525,  ..., -0.3074, -0.3792,  0.0161]]]],\n",
       "       grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_vec[torch.arange(top_vec.size(0)).unsqueeze(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2849,  0.0282,  0.2609,  ..., -0.0735, -0.0258,  0.3375],\n",
       "         [-0.0033, -0.2608, -0.1116,  ..., -0.0259, -0.1240,  0.1610],\n",
       "         [-0.0964, -0.1890,  0.0300,  ..., -0.0450, -0.0738,  0.2506],\n",
       "         [-0.1293, -0.1998,  0.0067,  ..., -0.0116, -0.0832,  0.2335],\n",
       "         [-0.0662, -0.2018,  0.0209,  ..., -0.0581, -0.0892,  0.2462],\n",
       "         [-0.0804, -0.2358, -0.0202,  ..., -0.0204, -0.1140,  0.1821]]],\n",
       "       grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_vec = top_vec[torch.arange(top_vec.size(0)).unsqueeze(1), clss]\n",
    "sents_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_vec = top_vec[torch.arange(top_vec.size(0)).unsqueeze(1), clss]\n",
    "        sents_vec = sents_vec * mask_cls[:, :, None].float()\n",
    "        sent_scores = self.ext_layer(sents_vec, mask_cls).squeeze(-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mecab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
